{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Summary/Analysis\n",
    "Use this notebook to calculate summary stats, correlation analyses\n",
    "and other useful metrics for training and evalution data.\n",
    "## Error Analysis\n",
    "Visualize the error between prediction and truth\n",
    "- For error analysis create the error dataset using _dumpdata.sc_ with the _--model_ argument.\n",
    "- (notebook) Set _ANALYSIS_ to 'error'\n",
    "- In the notebook set _ERROR_TRUTH_COL_ and _ERROR_PREDICTION_COL_ to the columns containing the true and predicted values\n",
    "## Summary\n",
    "Basic summary stats of the loaded dataframe statistics\n",
    "## Histogram\n",
    "Histograms showing distribution of statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "AnalysisType = Literal[\"summary\", \"error\", \"histogram\"]\n",
    "\n",
    "ANALYSES: list[AnalysisType]\n",
    "COLS_TO_DROP = None\n",
    "COLS_TO_ANALYZE = None\n",
    "FILTER_QUERY = None\n",
    "PREVIEW_DATA = False\n",
    "\"\"\"show a preview of data before analysis\"\"\"\n",
    "TARGET_PATTERN = None\n",
    "HISTOGRAM_COLS = 3\n",
    "HISTOGRAM_BINS = 10\n",
    "HISTOGRAM_INCHES_PER_ROW = 3\n",
    "HISTOGRAM_INCHES_PER_COL = 5\n",
    "DATA_LIMIT = 100000\n",
    "ERROR_TRUTH_COL = None\n",
    "ERROR_PREDICTION_COL = None\n",
    "ADDL_FEATURE_FUNC = None\n",
    "DROP_TARGET_OUTLIERS = True\n",
    "\n",
    "# DATA_FILES = glob(\n",
    "#     os.path.join(\"/\", \"fantasy-isync\", \"fantasy-modeling\", \"2023.12\", \"nhl_skater.pq\")\n",
    "# )\n",
    "# DATA_FILES = glob(os.path.join(\"/\", \"fantasy-isync\", \"fantasy-modeling\", \"2023.12\", \"nhl_goalie.pq\"))\n",
    "# DATA_FILES = glob(os.path.join(\"/\", \"fantasy-isync\", \"fantasy-modeling\", \"2023.12\", \"mlb_pitcher.pq\"))\n",
    "# DATA_FILES = glob(\n",
    "#     os.path.join(\"/\", \"fantasy-isync\", \"fantasy-modeling\", \"2023.12\", \"mlb_hitter.pq\")\n",
    "# )\n",
    "# DATA_FILES = glob(os.path.join(\"/\", \"fantasy-isync\", \"fantasy-modeling\", \"2023.12\", \"nba_player.pq\"))\n",
    "\n",
    "ANALYSES = [\"error\"]\n",
    "DATA_FILES = glob(os.path.join(\"/\", \"fantasy\", \"test-training-export.csv\"))\n",
    "ERROR_TRUTH_COL = \"target:calc:dk_score\"\n",
    "ERROR_PREDICTION_COL = \"calc:dk_score-std-mean\" #  \"prediction:calc:dk_score\"\n",
    "# ANALYSES = [\"histogram\", \"summary\"]\n",
    "# PREVIEW_DATA = True\n",
    "# TARGET_PATTERN = r\"calc:.*_score\"\n",
    "# COLS_TO_ANALYZE = r\"(calc:.*)|(((stat)|(extra)).*((recent-mean)|(std)).*)|\"\n",
    "# COLS_TO_ANALYZE = r\"(calc:.*)|(.*-mean.*)\"\n",
    "# COLS_TO_ANALYZE = r\"(stat|extra|calc):(?!venue).*\"\n",
    "\n",
    "# LOL dfs win score feature summary and correlation analysis\n",
    "# DATA_FILES = [os.path.join(\n",
    "#     \"/\", \"fantasy-experiments\", \"df-hist\", \"data\", \"lol-draftkings-CLASSIC-GPP.csv\"\n",
    "# )]\n",
    "# COLS_TO_DROP = [\"slate_id\", \"link\", \"style\", \"type\", \"date\"]\n",
    "# FILTER_QUERY = \"slate_id.notna()\"\n",
    "# ANALYSES = [\"summary\"]\n",
    "\n",
    "# DFS win score prediction error analysis\n",
    "# DATA_FILES = [\n",
    "#     filepath\n",
    "#     for filepath in glob(\n",
    "#         os.path.join(\n",
    "#             \"/\",\n",
    "#             \"fantasy-experiments\",\n",
    "#             \"df-hist\",\n",
    "#             \"eval_results\",\n",
    "#             \"*.prediction.csv\",\n",
    "#         )\n",
    "#     )\n",
    "# ]\n",
    "# ANALYSES = [\"error\"]\n",
    "\n",
    "\n",
    "def mlb_addl_features_func(row: pd.Series):\n",
    "    ret = {}\n",
    "    for sr in [\"std\", \"recent\"]:\n",
    "        ret[f\"addl:slug-{sr}-mean\"] = (\n",
    "            (\n",
    "                (\n",
    "                    row[f\"stat:off_1b:{sr}-mean\"]\n",
    "                    + 2 * row[f\"stat:off_2b:{sr}-mean\"]\n",
    "                    + 3 * row[f\"stat:off_3b:{sr}-mean\"]\n",
    "                    + 4 * row[f\"stat:off_hr:{sr}-mean\"]\n",
    "                )\n",
    "                / row[f\"stat:off_ab:{sr}-mean\"]\n",
    "            )\n",
    "            if row[f\"stat:off_ab:{sr}-mean\"] > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        obp_denom = (\n",
    "            row[f\"stat:off_ab:{sr}-mean\"]\n",
    "            + row[f\"stat:off_bb:{sr}-mean\"]\n",
    "            + row[f\"stat:off_sac:{sr}-mean\"]\n",
    "            + row[f\"stat:off_hbp:{sr}-mean\"]\n",
    "        )\n",
    "        ret[f\"addl:obp-{sr}-mean\"] = (\n",
    "            (\n",
    "                (\n",
    "                    row[f\"stat:off_hit:{sr}-mean\"]\n",
    "                    + row[f\"stat:off_bb:{sr}-mean\"]\n",
    "                    + row[f\"stat:off_hbp:{sr}-mean\"]\n",
    "                )\n",
    "                / obp_denom\n",
    "            )\n",
    "            if obp_denom > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        ret[f\"addl:ops-{sr}-mean\"] = ret[f\"addl:obp-{sr}-mean\"] + ret[f\"addl:slug-{sr}-mean\"]\n",
    "    return ret\n",
    "\n",
    "\n",
    "# ADDL_FEATURE_FUNC = mlb_addl_features_func\n",
    "\n",
    "assert len(DATA_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from math import sqrt\n",
    "from typing import Callable, Pattern\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def load(\n",
    "    path: str,\n",
    "    cols_to_drop: list[str] | None = None,\n",
    "    cols_to_analyze: Pattern[str] | None = None,\n",
    "    filter_query: str | None = None,\n",
    "    addl_features_func: Callable | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    filter_query: Rows not matching this query will be dropped\n",
    "    cols_to_drop: list of cols to not analyze\n",
    "    cols_to_analyze: list of cols to analyze. cols are names or regexs\n",
    "    \"\"\"\n",
    "    assert (cols_to_drop is None) or (\n",
    "        cols_to_analyze is None\n",
    "    ), \"cols_to_drop and cols_to_analyze cannot both be not None\"\n",
    "    if path.endswith(\".csv\"):\n",
    "        df = pd.read_csv(path)\n",
    "    elif path.endswith(\".pq\"):\n",
    "        df = pd.read_parquet(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown file type for path '{path}'\")\n",
    "    print(f\"Loaded {len(df)} rows\")\n",
    "    if DATA_LIMIT is not None:\n",
    "        df = df.head(DATA_LIMIT)\n",
    "    # if df.isna().any().any():\n",
    "    #     print(\"NAs found. filling na->0.0\")\n",
    "    #     df = df.fillna(0)\n",
    "\n",
    "    if ADDL_FEATURE_FUNC is not None:\n",
    "        cols = sorted(\n",
    "            {\":\".join(col.split(\":\", 2)[:2]) for col in df.columns if col[:4] in (\"stat\", \"calc\")}\n",
    "        )\n",
    "        print(f\"{cols=}\")\n",
    "\n",
    "        tqdm.pandas(desc=\"addl_features\")\n",
    "        addl_df = df.progress_apply(addl_features_func, result_type=\"expand\", axis=1)\n",
    "        addl_cols = list(addl_df.columns)\n",
    "        print(f\"applied addl_func. addl_cols = {addl_cols}\")\n",
    "        df = pd.concat([df, addl_df], axis=1)\n",
    "    else:\n",
    "        addl_cols = []\n",
    "\n",
    "    file_len = len(df)\n",
    "    if filter_query:\n",
    "        df = df.query(filter_query)\n",
    "        print(f\"Filter query dropped {file_len - len(df)} rows, {len(df)} remaining\")\n",
    "    if cols_to_drop is not None:\n",
    "        print(f\"Dropping columns: {cols_to_drop}\")\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "    if cols_to_analyze is not None:\n",
    "        cols_to_keep = [col for col in df.columns if re.match(cols_to_analyze, col)]\n",
    "        if (cols_dropped := len(df.columns) - len(cols_to_keep)) > 0:\n",
    "            print(\n",
    "                f\"Dropping {cols_dropped} columns, {len(cols_to_keep)} columns remaining in analysis. \"\n",
    "                f\"Dropped cols: {set(df.columns) - set(cols_to_keep)}\"\n",
    "            )\n",
    "            df = df[cols_to_keep]\n",
    "    print(f\"Loaded features: {sorted(list(df.columns))}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def summarize(df: pd.DataFrame, targets: list[str] | None):\n",
    "    if targets is not None:\n",
    "        no_target_df = df.drop(columns=targets)\n",
    "        corr_df = pd.DataFrame(\n",
    "            {target: no_target_df.corrwith(df[target]) for target in targets}\n",
    "        ).sort_values(targets, ascending=False)\n",
    "    else:\n",
    "        print(\"Running full cross correlation\")\n",
    "        corr_df = df.corr()\n",
    "\n",
    "    summary = {\n",
    "        \"summary\": df.describe().transpose().drop(columns=\"count\"),\n",
    "        \"correlation\": corr_df.style.background_gradient(axis=None, cmap=\"RdYlGn\"),\n",
    "    }\n",
    "    return summary\n",
    "\n",
    "\n",
    "def drop_target_outliers(df: pd.DataFrame, targets: set[str], outlier_range: float = 2.0):\n",
    "    \"\"\"drop rows with target outliers\"\"\"\n",
    "    df_no_outliers = df\n",
    "    for target in targets:\n",
    "        mean = df[target].mean()\n",
    "        std = df[target].std()\n",
    "        lower_bound = mean - outlier_range * std\n",
    "        upper_bound = mean + outlier_range * std\n",
    "        df = df[(df[target] >= lower_bound) & (df[target] <= upper_bound)]\n",
    "    if len(df) < len(df_no_outliers):\n",
    "        print(f\"Dropped {len(df_no_outliers) - len(df)} rows due to outliers\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def error_analysis(df: pd.DataFrame, desc, truth_col, pred_col):\n",
    "    assert truth_col in df.columns and pred_col in df.columns, \"truth or prediction col not found\"\n",
    "    error_df = df[[truth_col, pred_col]].dropna()\n",
    "    error_df[\"error\"] = error_df[truth_col] - error_df[pred_col]\n",
    "    error_df.rename(columns={truth_col: \"truth\", pred_col: \"prediction\"}, inplace=True)\n",
    "    r2 = round(metrics.r2_score(error_df.truth, error_df.prediction), 4)\n",
    "    rmse = round(sqrt(metrics.mean_squared_error(error_df.truth, error_df.prediction)), 4)\n",
    "    mae = round(sqrt(metrics.mean_absolute_error(error_df.truth, error_df.prediction)), 4)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    fig.suptitle(f\"{desc or 'unknown model'} : n={len(df)} {r2=} {rmse=} {mae=}\")\n",
    "    for ax in axs:\n",
    "        ax.axis(\"equal\")\n",
    "\n",
    "    min_v = min(error_df.truth.min(), error_df.prediction.min())\n",
    "    max_v = max(error_df.truth.max(), error_df.prediction.max())\n",
    "\n",
    "    axs[0].plot((min_v, max_v), (min_v, max_v), \"-g\", linewidth=1)\n",
    "    error_df.plot(kind=\"scatter\", x=\"truth\", y=\"prediction\", ax=axs[0])\n",
    "\n",
    "    axs[1].yaxis.set_label_position(\"right\")\n",
    "    axs[1].plot((min_v, max_v), (0, 0), \"-g\", linewidth=1)\n",
    "    error_df.plot(kind=\"scatter\", x=\"truth\", y=\"error\", ax=axs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filepath in DATA_FILES:\n",
    "    print(f\"Analyzing '{filepath}'\")\n",
    "    df = load(\n",
    "        filepath,\n",
    "        filter_query=FILTER_QUERY,\n",
    "        cols_to_drop=COLS_TO_DROP,\n",
    "        cols_to_analyze=COLS_TO_ANALYZE,\n",
    "        addl_features_func=ADDL_FEATURE_FUNC,\n",
    "    )\n",
    "\n",
    "    targets = (\n",
    "        [col for col in df.columns if re.match(TARGET_PATTERN, col)] if TARGET_PATTERN else None\n",
    "    )\n",
    "\n",
    "    if DROP_TARGET_OUTLIERS:\n",
    "        assert (\n",
    "            targets or ERROR_TRUTH_COL\n",
    "        ), \"must specify outlier_cols or ERROR_TRUTH_COL to drop outliers\"\n",
    "        outlier_cols = set(targets) if targets is not None else {ERROR_TRUTH_COL}\n",
    "        assert outlier_cols <= set(df.columns), \"outlier_cols must be a subset of df.columns\"\n",
    "        df = drop_target_outliers(df, outlier_cols)\n",
    "\n",
    "    if PREVIEW_DATA:\n",
    "        display(f\"data preview total-n={len(df)}\", df.head(5).style.hide())\n",
    "\n",
    "    if \"summary\" in ANALYSES:\n",
    "        print(f\"{TARGET_PATTERN=} => {targets=}\")\n",
    "        assert (targets is not None and len(targets) > 0) == (TARGET_PATTERN is not None)\n",
    "        summary = summarize(df, targets)\n",
    "        with pd.option_context(\n",
    "            \"display.max_rows\", None, \"display.max_columns\", None, \"display.max_colwidth\", None\n",
    "        ):\n",
    "            for name, summary_df in summary.items():\n",
    "                display(name)\n",
    "                display(summary_df)\n",
    "\n",
    "    if \"error\" in ANALYSES:\n",
    "        assert (\n",
    "            ERROR_TRUTH_COL in df.columns and ERROR_PREDICTION_COL in df.columns\n",
    "        ), f\"{ERROR_TRUTH_COL=} or {ERROR_PREDICTION_COL=} not found in df\"\n",
    "        error_analysis(df, os.path.basename(filepath), ERROR_TRUTH_COL, ERROR_PREDICTION_COL)\n",
    "\n",
    "    if \"histogram\" in ANALYSES:\n",
    "        rows = 1 + (len(df.columns) // HISTOGRAM_COLS)\n",
    "        layout = rows, HISTOGRAM_COLS\n",
    "        figsize = (HISTOGRAM_COLS * HISTOGRAM_INCHES_PER_COL), (rows * HISTOGRAM_INCHES_PER_ROW)\n",
    "        print(f\"{layout=} {figsize=}\")\n",
    "        df.hist(bins=HISTOGRAM_BINS, layout=layout, figsize=figsize)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
