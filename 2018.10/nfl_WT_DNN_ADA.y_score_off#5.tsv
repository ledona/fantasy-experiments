{"calc_name": "keras", "calc_params": {"calc_stats": {"cur_opp_team_stats": ["def_block_fg", "def_block_punt", "def_block_xpt", "def_fumble_recov", "def_fumble_recov_tds", "def_int", "def_int_tds", "def_sacks", "def_safety", "def_tds", "op_passing_yds", "op_pts", "op_rushing_yds", "op_turnovers", "op_yds"], "extra_stats": [], "model_player_stat": "y_score_off#5", "model_team_stat": null, "player_stats": ["fumbles_lost", "receiving_rec", "receiving_tds", "receiving_twoptm", "receiving_yds", "tds"], "prev_opp_team_stats": [], "team_stats": ["passing_yds", "pts", "rushing_yds", "turnovers"]}, "impute": true, "models_path": "./MODELS_keras", "normalize": true, "player_pos": ["TE", "WR"]}, "datetime_utc": "20181023 014959", "db_id": 1, "db_path": "nfl.db", "fantasy_version": "v0.30.6-47-g423284c", "filename_prefix": "nfl_WT_DNN_ADA", "folds": 2, "hyper_dists": {"activation": {"cls": "HPCategoricalDist", "name": "activation", "values": ["linear", "relu", "tanh", "sigmoid"]}, "dropout": {"cls": "HPNumericDist", "dist_type": "float", "high": 0.7, "low": 0.3, "max_float_percision": 10, "name": "dropout"}, "hist_agg": {"cls": "HPConstantDist", "name": "hist_agg", "value": "none"}, "layers": {"cls": "HPNumericDist", "dist_type": "int", "high": 5, "increment": 1, "low": 1, "name": "layers"}, "learning_method": {"cls": "HPCategoricalDist", "name": "learning_method", "values": ["adagrad", "adadelta", "adam", "adamax", "nadam"]}, "n_cases": {"cls": "HPNumericDist", "dist_type": "int", "high": 9000, "increment": 1, "low": 100, "name": "n_cases"}, "n_features": {"cls": "HPConstantDist", "name": "n_features", "value": null}, "n_games": {"cls": "HPNumericDist", "dist_type": "int", "high": 7, "increment": 1, "low": 1, "name": "n_games"}, "steps": {"cls": "HPNumericDist", "dist_type": "int", "high": 1000, "increment": 100, "low": 100, "name": "steps"}, "units": {"cls": "HPNumericDist", "dist_type": "int", "high": 100, "increment": 1, "low": 20, "name": "units"}}, "random_seed": 1468513421, "resume_datetimes": null, "scoring": ["mae", "r2"], "search": {"bayes_init_pts": 7, "bayes_retry_cache": true, "iterations": 70, "method": "bayes", "pretend": false}, "search_bayes_scoring_method": "mae", "season_parts": ["REG"], "seasons": [2017, 2016, 2015, 2014, 2013, 2012, 2011, 2010]}
score_mae	score_r2	activation	dropout	hist_agg	layers	learning_method	n_cases	n_features	n_games	steps	units
-4.8962045	0.0186255	tanh	0.445303443	none	3	adam	3650	25	1	800	96
-4.4110456	0.2160731	linear	0.4315976324	none	2	nadam	8930	150	6	500	74
-4.501783	0.1868216	sigmoid	0.3642952712	none	1	adagrad	4590	100	4	200	85
-5.0727868	-0.149631	tanh	0.3354385393	none	4	adamax	533	50	2	900	59
-4.9142873	-0.0375081	sigmoid	0.6551333987	none	3	adam	396	100	4	1000	65
-4.6120987	0.1493304	linear	0.6406887943	none	4	nadam	7080	50	2	900	78
-4.7626455	0.0917471	relu	0.6928689781	none	2	nadam	8417	150	6	400	29
-4.4621333	0.2119484	linear	0.3799491094	none	2	nadam	9000	175	7	400	73
-4.6106066	0.1262155	linear	0.7	none	1	nadam	7985	25	1	1000	100
-4.4312617	0.2088849	linear	0.653858195	none	1	adamax	9000	150	6	300	87
-4.4328806	0.2066525	linear	0.3	none	2	nadam	9000	150	6	1000	38
-4.5311311	0.1879338	linear	0.5975705037	none	4	nadam	9000	150	6	100	100
-4.4657437	0.1914593	linear	0.3	none	1	adam	3918	150	6	1000	21
-4.4262418	0.2104329	linear	0.7	none	2	nadam	9000	125	5	100	100
-4.4535661	0.2046858	linear	0.7	none	1	adadelta	9000	150	6	100	100
-4.6272604	0.114552	tanh	0.6319990489	none	5	adagrad	3592	25	1	1000	74
-4.4127442	0.2062514	linear	0.5645969721	none	4	adagrad	5667	150	6	800	76
-4.4553013	0.2053145	linear	0.5041773102	none	5	adagrad	5562	150	6	800	85
-4.4000837	0.2091613	linear	0.7	none	2	adamax	9000	150	6	100	100
-4.3727361	0.2043403	linear	0.7	none	3	adagrad	7928	150	6	100	100
-4.4286987	0.1794915	linear	0.4174432687	none	1	adagrad	4810	75	3	300	44
-4.3932571	0.2050528	linear	0.7	none	2	adagrad	9000	150	6	100	100
-4.4483145	0.194041	linear	0.5550100665	none	3	adamax	4483	125	5	100	100
-4.6518645	0.1086637	linear	0.7	none	4	adagrad	4977	25	1	100	100
-4.4633258	0.2079987	linear	0.4839348848	none	3	adamax	7673	150	6	200	100
-4.4694851	0.1953882	linear	0.7	none	2	adamax	3689	125	5	100	100
-4.3837375	0.205824	linear	0.7	none	3	adagrad	9000	175	7	100	100
-6.5243705	-1.1629545	linear	0.7	none	3	adagrad	100	175	7	100	23
-4.5331395	0.1554544	tanh	0.5499825482	none	5	adagrad	6006	175	7	1000	20
-4.4167423	0.2030403	linear	0.5277050107	none	1	adagrad	5094	175	7	700	20
-4.437363	0.213024	linear	0.3	none	5	adagrad	8329	175	7	1000	20
-4.6394499	0.1148644	sigmoid	0.6241140639	none	5	adagrad	6497	25	1	800	51
-4.4079525	0.213718	linear	0.7	none	3	adagrad	8302	175	7	300	97
-4.4114362	0.2155198	linear	0.6064235666	none	1	adagrad	8005	150	6	600	57
-4.6591182	0.0936693	tanh	0.7	none	5	adagrad	9000	25	1	1000	20
-4.3876167	0.2153633	linear	0.7	none	1	adagrad	9000	175	7	100	100
-4.3463766	0.204267	linear	0.7	none	1	adagrad	9000	175	7	100	20
-4.3463766	0.204267	linear	0.7	none	1	adagrad	9000	175	7	100	20
-4.3308475	0.1997048	linear	0.7	none	1	adagrad	7132	150	6	100	20
-4.3915169	0.201989	linear	0.7	none	1	adagrad	6147	150	6	100	20
-4.4809736	0.1909723	linear	0.4940989321	none	1	adamax	5109	100	4	1000	20
-5.0336262	-0.0046018	sigmoid	0.7	none	5	adagrad	3412	175	7	100	20
-4.4010013	0.2175613	linear	0.3	none	1	adagrad	9000	175	7	100	20
-4.3372763	0.1989281	linear	0.7	none	1	adagrad	8241	150	6	100	20
-4.3761392	0.2043621	linear	0.7	none	1	adagrad	5657	125	5	900	20
-4.3417135	0.2057126	linear	0.7	none	1	adagrad	8686	175	7	100	20
-4.3424966	0.1899576	linear	0.7	none	1	adagrad	4492	100	4	400	20
-4.3484271	0.2008293	linear	0.7	none	1	adagrad	8375	150	6	100	20
-4.3150917	0.2017394	linear	0.7	none	1	adagrad	8308	150	6	100	20
-4.3463766	0.204267	linear	0.7	none	1	adagrad	9000	175	7	100	20
-4.3677758	0.2009766	linear	0.7	none	1	adagrad	8565	150	6	100	20
-4.3542274	0.1967321	linear	0.7	none	1	adagrad	6196	175	7	100	20
-4.3463766	0.204267	linear	0.7	none	1	adagrad	9000	175	7	100	20
-4.3496667	0.1956851	linear	0.7	none	1	adagrad	6240	175	7	100	20
-4.3511027	0.1985527	linear	0.7	none	1	adagrad	6307	175	7	100	20
-4.343508	0.2039382	linear	0.7	none	1	adagrad	8627	150	6	100	20
-4.3463766	0.204267	linear	0.7	none	1	adagrad	9000	175	7	100	20
-4.3224236	0.1989801	linear	0.7	none	1	adagrad	8267	150	6	100	20
-4.3640479	0.19764	linear	0.7	none	1	adagrad	8146	150	6	100	20
-4.3275661	0.1980784	linear	0.7	none	1	adagrad	7908	150	6	100	20
-4.325998	0.201808	linear	0.7	none	1	adagrad	8522	150	6	100	20
-4.3270138	0.2037568	linear	0.7	none	1	adagrad	8687	150	6	100	20
-4.3420076	0.2000814	linear	0.7	none	1	adagrad	8699	150	6	100	20
-4.3403591	0.1986238	linear	0.7	none	1	adagrad	8546	150	6	100	20
-4.3247234	0.2032101	linear	0.7	none	1	adagrad	8574	150	6	100	20
-4.3480271	0.1992683	linear	0.7	none	1	adagrad	8605	150	6	100	20
-4.3245675	0.2049947	linear	0.7	none	1	adagrad	8578	150	6	100	20
-4.3372732	0.1990338	linear	0.7	none	1	adagrad	8595	150	6	100	20
-4.3243676	0.2022006	linear	0.7	none	1	adagrad	8563	150	6	100	20
-4.3386869	0.198925	linear	0.7	none	1	adagrad	8572	150	6	100	20
