{"calc_name": "keras", "calc_params": {"calc_stats": {"cur_opp_team_stats": ["def_block_fg", "def_block_punt", "def_block_xpt", "def_fumble_recov", "def_fumble_recov_tds", "def_int", "def_int_tds", "def_sacks", "def_safety", "def_tds", "op_passing_yds", "op_pts", "op_rushing_yds", "op_turnovers", "op_yds"], "extra_stats": [], "model_player_stat": "dk_score_off#3", "model_team_stat": null, "player_stats": ["fumbles_lost", "receiving_rec", "receiving_tds", "receiving_twoptm", "receiving_yds", "rushing_att", "rushing_tds", "rushing_twoptm", "rushing_yds", "tds"], "prev_opp_team_stats": [], "team_stats": ["passing_yds", "pts", "rushing_yds", "turnovers"]}, "impute": true, "models_path": "./MODELS_keras", "normalize": true, "player_pos": ["RB"]}, "datetime_utc": "20181022 115740", "db_id": 1, "db_path": "nfl.db", "fantasy_version": "v0.30.6-26-g58e3da5", "filename_prefix": "nfl_RB_DNN_ADA", "folds": 2, "hyper_dists": {"activation": {"cls": "HPCategoricalDist", "name": "activation", "values": ["linear", "relu", "tanh", "sigmoid"]}, "dropout": {"cls": "HPNumericDist", "dist_type": "float", "high": 0.7, "low": 0.3, "max_float_percision": 10, "name": "dropout"}, "hist_agg": {"cls": "HPConstantDist", "name": "hist_agg", "value": "none"}, "layers": {"cls": "HPNumericDist", "dist_type": "int", "high": 5, "increment": 1, "low": 1, "name": "layers"}, "learning_method": {"cls": "HPCategoricalDist", "name": "learning_method", "values": ["adagrad", "adadelta", "adam", "adamax", "nadam"]}, "n_cases": {"cls": "HPNumericDist", "dist_type": "int", "high": 3500, "increment": 1, "low": 100, "name": "n_cases"}, "n_features": {"cls": "HPConstantDist", "name": "n_features", "value": null}, "n_games": {"cls": "HPNumericDist", "dist_type": "int", "high": 7, "increment": 1, "low": 1, "name": "n_games"}, "steps": {"cls": "HPNumericDist", "dist_type": "int", "high": 1000, "increment": 100, "low": 100, "name": "steps"}, "units": {"cls": "HPNumericDist", "dist_type": "int", "high": 100, "increment": 1, "low": 20, "name": "units"}}, "random_seed": 1596061945, "resume_datetimes": null, "scoring": ["mae", "r2"], "search": {"bayes_init_pts": 7, "bayes_retry_cache": true, "iterations": 70, "method": "bayes", "pretend": false}, "search_bayes_scoring_method": "mae", "season_parts": ["REG"], "seasons": [2017, 2016, 2015, 2014, 2013, 2012, 2011, 2010]}
score_mae	score_r2	activation	dropout	hist_agg	layers	learning_method	n_cases	n_features	n_games	steps	units
-5.9958068	0.0697115	tanh	0.4506421968	none	1	adagrad	881	203	7	200	39
-5.7244797	0.2181787	linear	0.638818359	none	4	nadam	1218	203	7	1000	71
-5.3859689	0.2777662	linear	0.6425146061	none	2	adadelta	2333	203	7	100	65
-5.2552047	0.270625	linear	0.6794539375	none	1	adagrad	1000	58	2	700	40
-5.7031487	0.179248	tanh	0.6970527595	none	4	adadelta	2641	174	6	500	56
-5.7645821	0.0354097	relu	0.6503523275	none	4	adadelta	991	29	1	400	50
-5.6378897	0.0765	relu	0.536058873	none	3	adamax	879	145	5	600	88
-5.245236	0.2708578	linear	0.3780177206	none	1	adamax	1677	29	1	800	32
-5.2827417	0.2639688	linear	0.3	none	1	adamax	3500	29	1	1000	20
-5.4522539	0.2391492	linear	0.3	none	2	adamax	1165	29	1	800	20
-6.3479037	-0.3030703	linear	0.7	none	1	adamax	100	116	4	100	83
-5.4262259	0.2240854	linear	0.7	none	1	adagrad	1669	29	1	300	85
-6.2334242	0.0808779	linear	0.464370278	none	1	adamax	100	29	1	600	45
-5.224326	0.2690596	linear	0.3491743003	none	1	adamax	1814	29	1	800	28
-5.3642259	0.2559187	linear	0.3	none	1	adamax	2035	29	1	800	20
-5.7397861	0.1167636	linear	0.4423601294	none	4	adagrad	386	174	6	200	100
-5.2636112	0.2994988	linear	0.6995332304	none	2	adam	3360	87	3	700	100
-5.2139336	0.2892617	linear	0.450898398	none	2	adagrad	1158	145	5	1000	50
-5.6194159	0.2637093	linear	0.3197866719	none	3	adagrad	1164	116	4	1000	65
-5.4803141	0.2418578	linear	0.5906903888	none	2	adagrad	1148	87	3	1000	56
-5.2384865	0.2933543	linear	0.4712365662	none	1	adamax	3109	116	4	1000	64
-5.3584121	0.2774327	linear	0.5678456361	none	5	nadam	3081	116	4	1000	41
-5.6965609	0.1890522	linear	0.3	none	1	adagrad	1309	203	7	1000	20
-5.2903091	0.2551294	linear	0.3	none	1	adadelta	2797	29	1	300	100
-5.462561	0.2669079	linear	0.3	none	5	adadelta	3500	203	7	100	100
-5.3856835	0.2454795	linear	0.7	none	1	adamax	3500	29	1	1000	20
-5.1889586	0.2892458	linear	0.4732798338	none	1	adamax	3116	116	4	1000	97
-5.6629629	0.2304292	linear	0.5351609555	none	1	adagrad	860	145	5	500	20
-5.3317363	0.2541832	linear	0.5738103242	none	1	adamax	2661	29	1	900	96
-5.3396832	0.2636738	linear	0.6390297134	none	1	adagrad	3304	29	1	1000	62
-5.2884984	0.2633194	linear	0.5787999494	none	1	adamax	2836	29	1	600	39
-5.3789381	0.2549264	linear	0.3426809674	none	1	adamax	2829	29	1	300	42
-5.3054535	0.2500021	linear	0.3025547403	none	1	adamax	2971	29	1	900	70
-5.3498332	0.2735321	linear	0.5438729856	none	1	adamax	3013	29	1	800	91
-5.3863115	0.2555865	linear	0.5014555861	none	1	adamax	3500	29	1	800	76
-5.5157587	0.204211	linear	0.3454467795	none	2	adagrad	1155	174	6	1000	44
-5.3512677	0.2512331	linear	0.5247025509	none	1	adamax	2958	29	1	1000	26
-5.324772	0.2582469	linear	0.5537190889	none	1	adam	3009	29	1	1000	97
-5.3505566	0.2587336	linear	0.5079117532	none	1	adagrad	3500	29	1	700	91
-5.3448455	0.2585518	linear	0.581375336	none	1	adam	3090	29	1	1000	100
-5.1492262	0.2930748	linear	0.5032254569	none	1	adamax	3129	58	2	500	94
-5.3015587	0.2935657	linear	0.5209889287	none	1	adamax	3500	145	5	700	84
-5.3077925	0.2832169	linear	0.4206825995	none	1	adamax	3225	145	5	1000	100
-5.3038809	0.2734105	linear	0.5287543791	none	1	adagrad	3500	29	1	800	94
-5.2976522	0.2730514	linear	0.6351396197	none	1	adagrad	3500	29	1	1000	100
-5.2222214	0.3097118	linear	0.3445890424	none	1	adamax	3160	87	3	700	70
-5.2280475	0.2971817	linear	0.4743949789	none	1	adagrad	3500	87	3	1000	100
-5.3486905	0.2793538	linear	0.3	none	1	adagrad	3500	203	7	1000	100
-5.2511808	0.2789971	linear	0.3143303692	none	1	adagrad	3500	116	4	1000	100
-5.3001705	0.2920294	linear	0.3	none	1	adagrad	3500	145	5	1000	100
-5.3001705	0.2920294	linear	0.3	none	1	adagrad	3500	145	5	1000	100
-5.3001705	0.2920294	linear	0.3	none	1	adagrad	3500	145	5	1000	100
-5.3600863	0.2794091	linear	0.3	none	1	adagrad	3500	174	6	1000	100
-5.2677587	0.2944081	linear	0.3	none	1	adagrad	3500	116	4	1000	100
-5.2406596	0.290492	linear	0.5494438476	none	1	adam	3187	87	3	100	100
-5.2280397	0.3028094	linear	0.7	none	1	adamax	3017	87	3	800	74
-5.3966183	0.2570177	linear	0.3	none	1	nadam	1535	116	4	800	33
-5.5356961	0.2422933	sigmoid	0.3	none	1	nadam	3097	116	4	100	100
-5.4572373	0.2293176	sigmoid	0.3786294304	none	5	nadam	3500	29	1	500	24
-5.2634536	0.2880366	linear	0.3	none	1	adagrad	3500	145	5	700	100
-5.7906761	0.1759361	tanh	0.3	none	1	adadelta	3205	87	3	400	100
-5.3463694	0.287471	linear	0.3133044045	none	1	adagrad	3500	203	7	300	99
-5.3491091	0.287254	linear	0.4155726881	none	1	adagrad	3500	203	7	1000	100
-5.3491859	0.2767544	linear	0.7	none	1	adagrad	3500	174	6	800	100
-5.1318427	0.3000532	linear	0.3	none	2	adamax	3255	87	3	1000	100
-5.2627658	0.2848987	linear	0.3	none	2	adamax	3248	87	3	1000	100
-5.3001705	0.2920294	linear	0.3	none	1	adagrad	3500	145	5	1000	100
-5.3001705	0.2920294	linear	0.3	none	1	adagrad	3500	145	5	1000	100
-5.308843	0.2926949	linear	0.3	none	1	adagrad	3500	145	5	900	100
-5.3001705	0.2920294	linear	0.3	none	1	adagrad	3500	145	5	1000	100
