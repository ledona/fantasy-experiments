{"calc_name": "keras", "calc_params": {"calc_stats": {"cur_opp_team_stats": ["brnk", "d", "dmg", "drgk", "fb", "g", "gl", "k", "tk", "w", "wd", "wdk"], "extra_stats": ["modeled_stat_std_mean", "modeled_stat_trend"], "model_player_stat": null, "model_team_stat": "fd_score#1", "player_stats": [], "prev_opp_team_stats": [], "team_stats": ["brnk", "d", "dmg", "drgk", "fb", "g", "gl", "k", "tk", "w", "wd", "wdk"]}, "impute": true, "models_path": "./MODELS_keras", "normalize": true, "player_pos": []}, "datetime_utc": "20200521 221855", "db_id": 5, "db_path": "lol_hist_2016-2019.scored.db", "fantasy_version": "v2020.5.21", "filename_prefix": "lol-team-DNN_ADA", "folds": 3, "hyper_dists": {"activation": {"cls": "HPCategoricalDist", "name": "activation", "values": ["linear", "relu", "tanh", "sigmoid"]}, "dropout": {"cls": "HPNumericDist", "dist_type": "float", "high": 0.7, "low": 0.3, "max_float_percision": 10, "name": "dropout"}, "hist_agg": {"cls": "HPCategoricalDist", "name": "hist_agg", "values": ["mean", "median", "none"]}, "layers": {"cls": "HPNumericDist", "dist_type": "int", "high": 5, "increment": 1, "low": 1, "name": "layers"}, "learning_method": {"cls": "HPCategoricalDist", "name": "learning_method", "values": ["adagrad", "adadelta", "adam", "adamax", "nadam"]}, "n_cases": {"cls": "HPNumericDist", "dist_type": "int", "high": 11000, "increment": 1, "low": 500, "name": "n_cases"}, "n_features": {"cls": "HPConstantDist", "name": "n_features", "value": null}, "n_games": {"cls": "HPNumericDist", "dist_type": "int", "high": 7, "increment": 1, "low": 1, "name": "n_games"}, "steps": {"cls": "HPNumericDist", "dist_type": "int", "high": 1000, "increment": 100, "low": 100, "name": "steps"}, "units": {"cls": "HPNumericDist", "dist_type": "int", "high": 100, "increment": 1, "low": 20, "name": "units"}}, "model_name": null, "random_seed": 1104494603, "resume_datetimes": null, "scoring": ["mae", "r2"], "search": {"bayes_fail_fast": false, "bayes_init_pts": 7, "bayes_retry_cache": true, "iterations": 70, "method": "bayes", "pretend": false}, "search_bayes_scoring_method": "mae", "season_parts": ["REG"], "seasons": [2016, 2017, 2018, 2019], "sport_pt_filter_flags": {}}
score_mae	score_r2	activation	dropout	hist_agg	layers	learning_method	n_cases	n_features	n_games	steps	units
-7.4137302	0.044377	linear	0.5909865306	none	4	adam	2271	98	4	800	80
-7.3710145	0.0063952	tanh	0.4295992196	none	1	adadelta	8624	122	5	400	55
-7.554977	-0.0559437	relu	0.4290860539	none	4	nadam	10238	98	4	300	72
-7.1947404	0.077769	tanh	0.3518045486	median	4	adagrad	10184	26	3	700	75
-7.311205	0.0665721	linear	0.3060256862	mean	4	adam	8961	26	2	800	69
-7.2860083	0.0778786	linear	0.5855132346	none	4	adamax	10033	74	3	900	64
-7.166764	0.0835475	linear	0.4045043851	mean	3	adam	5542	26	3	900	44
-7.3418433	0.064732	linear	0.5016787221	mean	2	adam	923	26	3	1000	20
-7.7386634	-0.0737944	relu	0.4012809073	mean	5	adam	4165	26	3	300	85
-7.3175511	0.0664678	linear	0.3	mean	3	adam	11000	26	1	900	38
-7.3150858	0.0672364	linear	0.4048274452	mean	1	adam	11000	26	1	1000	20
-7.2921196	0.0506432	tanh	0.3	mean	5	adagrad	11000	26	1	1000	100
-7.2907464	0.0841226	linear	0.7	mean	1	adam	5239	26	3	1000	20
-7.2083734	0.0810475	tanh	0.6154745909	median	1	adagrad	6030	26	7	100	20
-7.2229402	0.0694606	relu	0.6070574366	none	1	adagrad	9040	170	7	500	31
-8.1933217	-0.2622543	sigmoid	0.7	median	1	adagrad	500	26	7	100	20
-7.3758174	0.051121	linear	0.3	mean	5	adam	5886	26	1	1000	100
-7.3327388	0.0837527	linear	0.3	mean	5	adamax	5346	26	6	900	27
-7.4866661	-0.0496015	sigmoid	0.3554246221	mean	5	nadam	3330	26	1	400	94
-7.2345156	0.0771997	tanh	0.4529098956	median	3	adagrad	8587	26	4	500	56
-8.6614668	-0.4647499	relu	0.3	median	5	adagrad	500	26	1	1000	100
-7.2937583	0.0727875	tanh	0.4974496799	median	4	adagrad	8578	26	4	600	48
-7.2526945	0.082535	linear	0.7	mean	1	adagrad	11000	26	7	1000	20
-7.1223112	0.0974919	linear	0.4083783143	mean	2	adadelta	6397	26	6	900	44
-7.1779308	0.0855966	linear	0.3631408313	none	2	adamax	9028	170	7	800	46
-7.145984	0.0855605	linear	0.4565607855	none	2	adamax	4070	146	6	900	48
-7.2250087	0.0855141	linear	0.7	mean	1	adagrad	6300	26	7	1000	47
-7.2040205	0.0850327	tanh	0.3	median	1	adagrad	11000	26	4	300	100
-7.1976407	0.0954952	linear	0.3830112284	median	3	adamax	5969	26	7	1000	20
-7.1458296	0.0998227	linear	0.4296974563	mean	2	adamax	5896	26	7	1000	20
-7.2209587	0.0744657	linear	0.3	none	2	adagrad	5060	170	7	100	100
-7.1599348	0.0827812	linear	0.3591388283	none	2	adagrad	5261	170	7	700	62
-7.2688426	0.0776938	sigmoid	0.7	none	1	adagrad	11000	170	7	1000	20
-7.3753853	0.0549152	linear	0.3442495028	none	2	nadam	4323	98	4	200	53
-7.2169275	0.0753137	linear	0.3760491367	none	2	adagrad	4679	74	3	900	88
-7.1343158	0.0931008	linear	0.4874627749	mean	3	adadelta	5800	26	7	100	47
-7.3550015	0.065893	linear	0.5271163253	none	2	adamax	4214	98	4	400	23
-7.1628099	0.0944222	linear	0.3	mean	1	adadelta	11000	26	7	1000	100
-7.1305701	0.090733	linear	0.3208411341	mean	2	adadelta	8453	26	7	900	76
-7.2603493	0.0833734	linear	0.3	mean	3	adadelta	6597	26	7	1000	100
-7.1826991	0.0936741	linear	0.3	mean	1	adadelta	11000	26	7	1000	20
-7.1628099	0.0944222	linear	0.3	mean	1	adadelta	11000	26	7	1000	100
-7.1319661	0.0961442	linear	0.3779661779	mean	2	adagrad	5375	26	6	900	32
-7.1648194	0.0791119	linear	0.4460045346	mean	2	adadelta	4479	26	5	900	98
-7.2380303	0.0825854	linear	0.4090834813	none	2	adadelta	6565	170	7	800	69
-7.1578132	0.0934011	linear	0.3708001411	mean	1	adadelta	7776	26	7	400	100
-7.153912	0.0932678	linear	0.3	mean	1	adagrad	11000	26	7	700	100
-7.2938565	0.0776125	relu	0.6188068019	mean	3	adam	11000	26	7	100	53
-7.1706291	0.090824	linear	0.4023845375	mean	2	adamax	6255	26	7	500	29
-7.1627815	0.0942425	linear	0.4099866685	mean	1	adagrad	11000	26	7	1000	58
-7.1692833	0.0883865	linear	0.3290566223	mean	1	adagrad	5526	26	7	1000	58
-7.1979592	0.0908421	linear	0.3248939777	mean	2	adagrad	10304	26	7	800	58
-7.1406375	0.085312	linear	0.450462245	mean	5	adamax	8798	26	7	900	100
-7.2247234	0.0870509	linear	0.3883377745	mean	1	adagrad	3606	26	7	1000	21
-7.1644205	0.0905754	linear	0.4215160726	mean	2	adadelta	8855	26	7	500	43
-7.0810051	0.0984019	linear	0.4910900529	mean	2	adadelta	4769	26	6	400	47
-7.1359571	0.0857118	linear	0.38512997	mean	2	adadelta	5244	26	6	300	51
-7.1258015	0.0922537	linear	0.3	mean	2	adadelta	5898	26	7	1000	98
-7.2497792	0.0877863	linear	0.4350711494	mean	2	adagrad	5965	26	6	700	45
-7.16839	0.0912882	linear	0.3358938372	mean	2	adadelta	9938	26	7	900	61
-7.1565496	0.0920834	linear	0.5291951064	mean	1	adadelta	10559	26	7	600	79
-7.2030085	0.0911138	linear	0.321171133	mean	2	adadelta	8135	26	7	900	85
-7.1684657	0.0842593	linear	0.3077130807	mean	2	adadelta	6885	26	7	600	61
-7.0972942	0.0938224	linear	0.4242437218	mean	1	adadelta	6564	26	7	500	74
-7.114529	0.0909043	linear	0.3299473921	mean	2	adadelta	8457	26	7	500	85
-7.1625241	0.0878218	linear	0.474247169	mean	2	adadelta	4270	26	7	900	51
-7.3320806	0.0747605	linear	0.3576306263	mean	2	adadelta	3184	26	7	700	83
-7.176548	0.0932219	linear	0.5025038681	mean	1	adamax	7048	26	7	800	42
-7.0995223	0.0985234	linear	0.550379966	mean	2	adadelta	6660	26	7	900	39
-7.179818	0.0948482	linear	0.3909791863	mean	2	adadelta	6358	26	6	200	32
