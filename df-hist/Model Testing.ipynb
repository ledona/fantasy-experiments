{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "585d8aeb-9ef2-471c-a683-fb65c12007d9",
   "metadata": {},
   "source": [
    "# Model Testing for Daily Fantasy Scores\n",
    "Generate models that predict for the minimum and maximum winning scores for a slate.\n",
    "Log results to an evaluation result folder including evaluation results, model descriptions \n",
    "and model serializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354471f7-c002-40cd-b71f-f8fec169819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import log\n",
    "\n",
    "log.setup()\n",
    "\n",
    "LOGGER = logging.getLogger('Model.Testing')\n",
    "LOGGER.info(\"logger ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20794400-8eed-4153-814a-bd03ae63ed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from datetime import datetime\n",
    "import os\n",
    "from typing import Literal\n",
    "from pprint import pformat\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import onnx\n",
    "from pypmml import Model\n",
    "\n",
    "from generate_train_test import generate_train_test, load_csv\n",
    "from automl import create_automl_model, error_report, JpmmlModel, PMMLFileFramework\n",
    "from serialize import serialize_model, SerializeFailure\n",
    "\n",
    "\n",
    "EVAL_RESULT_COL_ORDER = [\n",
    "    'Sport', 'Service', 'Type', 'Style', 'Target', 'R2',\n",
    "    'RMSE', 'MAE', 'ModelType', 'Date', 'Params'\n",
    "]   \n",
    "\n",
    "def log_eval_results(eval_results: list[dict], name: str,\n",
    "                     csv_folder: str = \"eval_results\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    write all evaluation results to csv file in the temp folder and return the dataframe\n",
    "    also write file(s) that describe the final model(s)\n",
    "    \"\"\"\n",
    "    if len(eval_results) == 0:\n",
    "        LOGGER.warn(\"No evaluation results to save\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(eval_results)[EVAL_RESULT_COL_ORDER] \\\n",
    "        .sort_values(['Sport', 'Service', 'Type', 'Style', 'Target', 'ModelType'])\n",
    "    if not os.path.isdir(csv_folder):\n",
    "        os.mkdir(csv_folder)\n",
    "    results_filepath = os.path.join(csv_folder, name + \".csv\")\n",
    "    df.to_csv(results_filepath, index=False)\n",
    "\n",
    "    LOGGER.info(f\"Evaluation results written to '{results_filepath}\")\n",
    "    return df\n",
    "\n",
    "def finalize_results(\n",
    "    results: dict, \n",
    "    shared_results_dict, \n",
    "    target, \n",
    "    automl_params, \n",
    "    X_train, y_train,\n",
    "    model_desc: str = None,\n",
    "    results_folder: str = None,\n",
    "    serialize_format: str | None = None\n",
    ") -> tuple[dict, str]:\n",
    "    \"\"\" \n",
    "    update and return error results to include model information for a single model evaluation and \n",
    "    serialize the model and model description\n",
    "\n",
    "    results: dict of results from a single model evaluation from create_automl_model()\n",
    "    \"\"\"\n",
    "    finalized_results = results['eval_result'].copy()\n",
    "    finalized_results['Target'] = target\n",
    "    finalized_results['Params'] = automl_params.copy()\n",
    "    finalized_results.update(shared_results_dict)\n",
    "    model_filepath = None\n",
    "    if results_folder:\n",
    "        if model_desc is None:\n",
    "            raise ValueError(\"If result_folder is defined then model_desc must not be None\")\n",
    "        _, model_filepath = serialize_model(\n",
    "            results['model'], finalized_results['ModelType'], \n",
    "            X_train, y_train, \n",
    "            model_desc,\n",
    "            model_folder=results_folder,\n",
    "            model_desc_folder=results_folder,\n",
    "            output_format=serialize_format,\n",
    "        )\n",
    "    return finalized_results, model_filepath\n",
    "\n",
    "\n",
    "PMML_FILE_FRAMEWORK: PMMLFileFramework\n",
    "\n",
    "\n",
    "def validate_exported_model(\n",
    "    filepath, cam_result, model_desc, y_fallback, X_test, y_test,\n",
    "    r2_tolerance: float = None,\n",
    "):\n",
    "    LOGGER.info(\"loading exported model from '%s'\", filepath)\n",
    "    if filepath.endswith('onnx'):\n",
    "        model = onnx.load(filepath)\n",
    "        onnx.checker.check_model(model)\n",
    "        # print(\"inputs: \", [input.name for input in model.graph.input])\n",
    "        # print(\"outputs: \", [output.name for output in model.graph.output])    \n",
    "    elif filepath.endswith('pmml'):\n",
    "        if PMML_FILE_FRAMEWORK == 'pypmml':\n",
    "            model = Model.load(filepath)    \n",
    "        elif PMML_FILE_FRAMEWORK.startswith('jpmml'):\n",
    "            model = JpmmlModel(filepath, PMML_FILE_FRAMEWORK)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported PMML file framework: {PMML_FILE_FRAMEWORK}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown file extension for model file '{filepath}'\")\n",
    "\n",
    "    file_results, file_predictions = error_report(model, X_test, y_test, \n",
    "                                                  desc=filepath,\n",
    "                                                  y_fallback=y_fallback,\n",
    "                                                  show_results=False)\n",
    "    \n",
    "    if file_results is None:\n",
    "        raise SerializeFailure({\n",
    "            'cause': f\"Error calculating error metrics for exported model at '{filepath}'\", \n",
    "        })\n",
    "\n",
    "    if (r2_diff := abs(file_results['R2'] - cam_result['eval_result']['R2'])) > r2_tolerance:\n",
    "        raise SerializeFailure({\n",
    "            'cause': f\"R2 Difference between {model_desc} model in-memory and exported exceeds tolerance of {r2_tolerance}\",\n",
    "            \"r2_diff\": r2_diff,\n",
    "            \"eval_result\": cam_result['eval_result'],\n",
    "            \"file_result\": file_results,\n",
    "            \"eval_predictions\": cam_result['predictions'],\n",
    "            \"file_predictions\": file_predictions,\n",
    "        })\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaf623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from glob import iglob\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn2pmml.decoration import ContinuousDomain\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn2pmml import make_pmml_pipeline\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "models that can be evaluated :\n",
    "'all-top' - All available input data predicting the top score\n",
    "'all-lws' - All available input data predicting last winning score\n",
    "\"\"\"\n",
    "_VALID_MODELS = Literal[\n",
    "    'all-top', 'all-lws',\n",
    "]\n",
    "\n",
    "def targets_from_models_to_test(models_to_test: _VALID_MODELS) -> list[str]:\n",
    "    targets = []\n",
    "    for model_to_test in models_to_test:\n",
    "        if model_to_test == 'all-top':\n",
    "            targets.append('top-score')\n",
    "        elif model_to_test == 'all-lws':\n",
    "            targets.append('last-winning-score')\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model to test: {model_to_test}\")\n",
    "    return targets\n",
    "\n",
    "def evaluate_models(\n",
    "    sport, service, style, contest_type, framework, automl_params, pbar,\n",
    "    pca_components=5, data_folder=\"data\",\n",
    "    models_to_test: set[_VALID_MODELS] | None = None,\n",
    "    results_folder: str = None,\n",
    "    overwrite=False,\n",
    "    serialize_format=None,\n",
    "    r2_tolerance: float = None,\n",
    ") -> tuple[dict, list, list[tuple[str, str]]]:\n",
    "    \"\"\"\n",
    "    models_to_test - set/list of the models to test. if None then all models tested. possible models are\n",
    "    returns tuple of (models, evaluation results, failed models)\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    eval_results = []\n",
    "    shared_results_dict = {\n",
    "        'Sport': sport,\n",
    "        'Service': service,\n",
    "        'Style': style.name,\n",
    "        'Type': contest_type.NAME,\n",
    "        'ModelType': framework,\n",
    "        'Date': datetime.now().strftime(\"%Y%m%d\"),\n",
    "    }\n",
    "    failed_models: tuple[str, dict] = []\n",
    "\n",
    "    if models_to_test is None:\n",
    "        final_models_to_test = {'all-top', 'all-lws'}\n",
    "    else:\n",
    "        final_models_to_test = models_to_test\n",
    "\n",
    "    try: \n",
    "        df = load_csv(sport, service, style, contest_type, data_folder=data_folder)\n",
    "    except FileNotFoundError as ex:\n",
    "        LOGGER.error(\n",
    "            f\"Data file required for modeling not found {ex}. Skipping %s-%s-%s-%s\",\n",
    "            sport, service, style.name, contest_type.NAME\n",
    "        )\n",
    "        pbar.update(len(final_models_to_test))\n",
    "        failed_models = [\n",
    "            (f\"{sport}-{service}-{style.name}-{contest_type.NAME}-{target}-{framework}\", {'cause': \"No data file found\"})\n",
    "            for target in targets_from_models_to_test(final_models_to_test)\n",
    "        ]\n",
    "        return None, None, failed_models\n",
    "\n",
    "    model_data = generate_train_test(\n",
    "        df,\n",
    "        model_cols=None,\n",
    "        random_state=automl_params['random_state'],\n",
    "    )\n",
    "\n",
    "    if model_data is None or len(model_data[0]) < 5:\n",
    "        LOGGER.error(\n",
    "            \"Not enough training data available! Only found %i training cases. Skipping all training\",\n",
    "            len(model_data[0]) if model_data else 0\n",
    "        )\n",
    "        pbar.update(len(final_models_to_test))\n",
    "        failed_models = [\n",
    "            (f\"{sport}-{service}-{style.name}-{contest_type.NAME}-{target}-{framework}\", {'cause': \"Insufficient data\"})\n",
    "            for target in targets_from_models_to_test(final_models_to_test)\n",
    "        ]\n",
    "        return None, None, failed_models\n",
    "\n",
    "    (X_train, X_test, y_top_train, y_top_test,\n",
    "        y_last_win_train, y_last_win_test) = model_data\n",
    "\n",
    "    model_ys = []\n",
    "    if 'all-top' in final_models_to_test:\n",
    "        model_ys.append(('top-score', y_top_train, y_top_test))\n",
    "    if 'all-lws' in final_models_to_test:\n",
    "        model_ys.append(('last-win-score', y_last_win_train, y_last_win_test))\n",
    "        \n",
    "    if len(model_ys) == 0:\n",
    "        raise ValueError(f\"No models to test: {final_models_to_test}\")\n",
    "\n",
    "    # models for top and last winning score\n",
    "    for target, y_train, y_test in model_ys:\n",
    "        model_desc = f\"{sport}-{service}-{style.name}-{contest_type.NAME}-{target}-{framework}\"\n",
    "        if results_folder:\n",
    "            result_filepath = os.path.join(results_folder, model_desc + \".csv\")\n",
    "            if not overwrite and os.path.isfile(result_filepath):\n",
    "                LOGGER.info(\"Found results file '%s', skipping\", result_filepath)\n",
    "                existing_result = pd.read_csv(result_filepath).to_dict('records')[0]\n",
    "                eval_results.append(existing_result)\n",
    "                pbar.update()\n",
    "                continue\n",
    "            for old_failed_file in iglob(os.path.join(results_folder, model_desc + \"*.failed\")):\n",
    "                LOGGER.info(\"deleting old failed file '%s'\", old_failed_file)\n",
    "                os.remove(old_failed_file)\n",
    "\n",
    "        LOGGER.info(\"training model=%s\", model_desc)\n",
    "        pbar.set_postfix_str(model_desc)\n",
    "\n",
    "        # TODO: this should be in create_automl_model() or in some other preprocessing function\n",
    "        if serialize_format == 'pmml':\n",
    "            # pre feature eng step\n",
    "            feature_eng_pipeline = Pipeline([\n",
    "                (\n",
    "                    \"mapper\", \n",
    "                    DataFrameMapper([\n",
    "                        ([col], ContinuousDomain()) for col in X_train.columns\n",
    "                    ])\n",
    "                )\n",
    "            ])\n",
    "            # fit against the output of the transformation\n",
    "            Xt = feature_eng_pipeline.fit_transform(X_train)\n",
    "            Xt = Xt.astype(float)\n",
    "        else:\n",
    "            Xt = X_train\n",
    "                    \n",
    "        # TODO: this should be in serialize\n",
    "        if serialize_format == 'pmml':\n",
    "            def post_process(model):        \n",
    "                if framework == 'tpot':\n",
    "                    model = model.fitted_pipeline_\n",
    "                pipeline = Pipeline(feature_eng_pipeline.steps + model.steps)\n",
    "                model = make_pmml_pipeline(\n",
    "                    pipeline, active_fields=X_train.columns, target_fields=[y_train.name]\n",
    "                )\n",
    "                return model\n",
    "        else:\n",
    "            post_process = None\n",
    "            \n",
    "        cam_result = create_automl_model(\n",
    "            target,\n",
    "            framework=framework,\n",
    "            X_train=Xt,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            model_desc=model_desc,\n",
    "            target_output=serialize_format,\n",
    "            post_process_model=post_process,\n",
    "            **automl_params\n",
    "        )\n",
    "\n",
    "        models[model_desc] = cam_result['model']\n",
    "        pbar.update()\n",
    "\n",
    "        model_desc_filepath = None\n",
    "        try:\n",
    "            results, model_filepath = finalize_results(\n",
    "                cam_result, shared_results_dict, target,\n",
    "                automl_params,\n",
    "                X_train, y_train,\n",
    "                model_desc=model_desc,\n",
    "                results_folder=results_folder,\n",
    "                serialize_format=serialize_format,\n",
    "            )\n",
    "            if results_folder:\n",
    "                log_eval_results([results], model_desc, csv_folder=results_folder)\n",
    "                validate_exported_model(\n",
    "                    model_filepath, cam_result, model_desc, np.median(y_train),\n",
    "                    X_test, y_test, r2_tolerance=r2_tolerance,\n",
    "                )\n",
    "            eval_results.append(results)\n",
    "        except Exception as e:\n",
    "            LOGGER.error(\"Failure during serialization of model %s\", model_desc, exc_info=e)\n",
    "            failed_models.append((model_desc, e.args[0]))\n",
    "            if results_folder:\n",
    "                if os.path.isfile(result_filepath):\n",
    "                    pattern = os.path.join(results_folder, model_desc + \"*\")\n",
    "                    LOGGER.info(\"Due to serialization failure, renaming model result files '%s' -> '%s.failed\", \n",
    "                                pattern, pattern)\n",
    "                    for failed_file in iglob(pattern):\n",
    "                        if failed_file.endswith(\".failed\"):\n",
    "                            continue\n",
    "                        os.rename(failed_file, failed_file + \".failed\")\n",
    "                else:\n",
    "                    LOGGER.info(\"Touching fail file '%s.failed\", result_filepath)\n",
    "                    Path(result_filepath + \".failed\").touch()\n",
    "            if not isinstance(e, SerializeFailure):\n",
    "                raise\n",
    "\n",
    "    return models, eval_results, failed_models\n",
    "\n",
    "\n",
    "def multi_run(\n",
    "    model_params: dict, styles, sports, services, contest_types,\n",
    "    models_to_test: set[_VALID_MODELS] | None = None,\n",
    "    results_folder: str = None,\n",
    "    serialize_format='pmml',\n",
    "    r2_tolerance=None,\n",
    "):\n",
    "    LOGGER.info(\"starting multirun\")\n",
    "    models = {}\n",
    "    eval_results = []\n",
    "    progress_total = len(sports) * len(styles) * len(services) * len(contest_types) * len(model_params) * \\\n",
    "        (len(models_to_test) if models_to_test else 6)\n",
    "    pbar = tqdm(total=progress_total, desc=\"Modeling\")\n",
    "    all_failed_models = []\n",
    "    try:\n",
    "        for (\n",
    "            sport, service, style, contest_type, (framework, params)\n",
    "        ) in product(\n",
    "            sports, services, styles, contest_types, model_params.items()\n",
    "        ):\n",
    "            (new_models, new_eval_results, failed_models) = evaluate_models(\n",
    "                sport, service, style, contest_type, framework, params, pbar,\n",
    "                models_to_test=models_to_test, \n",
    "                results_folder=results_folder,\n",
    "                serialize_format=serialize_format,\n",
    "                r2_tolerance=r2_tolerance,\n",
    "            )\n",
    "            all_failed_models += failed_models\n",
    "            if new_models is None:\n",
    "                LOGGER.warning(\"No models generated for %s-%s-%s-%s\",\n",
    "                                sport, service, style.name, contest_type.NAME)\n",
    "            else:\n",
    "                models.update(new_models)\n",
    "                eval_results += new_eval_results\n",
    "    except Exception as ex:\n",
    "        LOGGER.error(\"Unhandled exception! \", exc_info=ex)\n",
    "        return models, eval_results, ex, all_failed_models\n",
    "    LOGGER.info(\"finished multirun.\")\n",
    "    pbar.close()\n",
    "    return models, eval_results, None, all_failed_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3a954a-f170-46a3-88de-3741d31a721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from fantasy_py import ContestStyle\n",
    "from fantasy_py.lineup.strategy import GeneralPrizePool, FiftyFifty\n",
    "\n",
    "SPORTS = ['nfl']  # ['nhl', 'nfl', 'mlb', 'nba', 'lol']\n",
    "STYLES = [ContestStyle.CLASSIC] # [ContestStyle.CLASSIC, ContestStyle.SHOWDOWN]\n",
    "RANDOM_SEED = 0\n",
    "SERVICES = ['fanduel'] # ['fanduel', 'draftkings', 'yahoo']\n",
    "CONTEST_TYPES = [GeneralPrizePool] # [FiftyFifty, GeneralPrizePool]\n",
    "MODELS_TO_TEST = {'all-top'} # {'all-top', 'all-lws'}\n",
    "SERIALIZE_FORMAT = 'pmml'\n",
    "PMML_FILE_FRAMEWORK = 'jpmml-file'\n",
    "R2_TOLERANCE = 0.25\n",
    "\n",
    "AUTOML_PARAMS = {\n",
    "    # TODO: try skautoml once onnx export is possible\n",
    "    # 'skautoml': {\n",
    "    #     'per_run_time_limit': 120,\n",
    "    #     'max_train_time': 900,\n",
    "    #     'n_jobs': 4,\n",
    "    #     'random_state': RANDOM_SEED,\n",
    "    # },\n",
    "    'tpot': {\n",
    "        'population_size': 100,\n",
    "        'n_jobs': 6,\n",
    "        'verbosity': 2,\n",
    "        'max_train_time': 1200,\n",
    "        'generations': 2, # 100,\n",
    "        'early_stop': 15,\n",
    "        'template': 'Selector-Transformer-Regressor',\n",
    "        'random_state': RANDOM_SEED,\n",
    "    }\n",
    "}\n",
    "\n",
    "(models, eval_results, unhandled_exception, failed_models) = multi_run(\n",
    "    AUTOML_PARAMS, STYLES, SPORTS, SERVICES, CONTEST_TYPES, \n",
    "    models_to_test=MODELS_TO_TEST,\n",
    "    results_folder='eval_results',\n",
    "    serialize_format=SERIALIZE_FORMAT,\n",
    "    r2_tolerance=R2_TOLERANCE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93df427",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_col_order = [\n",
    "    'Sport', 'Service', 'Type', 'Style', 'Target', 'R2',\n",
    "    'RMSE', 'MAE', 'ModelType', 'Date', 'Params'\n",
    "]\n",
    "\n",
    "print(f\"{len(eval_results) + len(failed_models)} models evaluated, {len(failed_models)} failed, {len(eval_results)} successful\")\n",
    "for n, failure in enumerate(failed_models):\n",
    "    print(f\"failure #{n + 1}: {failure[0]}\\n\\tcause='{failure[1]['cause']}'\")\n",
    "\n",
    "\n",
    "if len(eval_results):\n",
    "    eval_results_df = pd.DataFrame(eval_results)[eval_result_col_order] \\\n",
    "        .sort_values(['Sport', 'Service', 'Type', 'Style', 'Target', 'ModelType'])\n",
    "\n",
    "    eval_results_df = log_eval_results(\n",
    "        eval_results,\n",
    "        \"all_eval_results\"\n",
    "    )\n",
    "\n",
    "    if unhandled_exception:\n",
    "        import traceback\n",
    "        print(traceback.format_exc(limit=None, chain=True))\n",
    "\n",
    "    print(f\"{len(eval_results)} successfully serialized models\")\n",
    "    with pd.option_context(\n",
    "        'display.max_rows', 1000,\n",
    "        'display.max_columns', 100,\n",
    "        'display.max_colwidth', None\n",
    "    ):\n",
    "        display(eval_results_df)\n",
    "\n",
    "    print(eval_results_df.to_csv(index=False, sep=\"\\t\"))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "006d5deb8e6cdcd4312641bdf15f3bc20f0769a7305d81173599a7b40f33b4a2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "006d5deb8e6cdcd4312641bdf15f3bc20f0769a7305d81173599a7b40f33b4a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
