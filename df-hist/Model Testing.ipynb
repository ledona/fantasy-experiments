{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "585d8aeb-9ef2-471c-a683-fb65c12007d9",
   "metadata": {},
   "source": [
    "# Model Testing for Daily Fantasy Scores\n",
    "Generate models that predict for the minimum and maximum winning scores for a slate.\n",
    "Log results to an evaluation result folder including evaluation results, model descriptions \n",
    "and model serializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354471f7-c002-40cd-b71f-f8fec169819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import log\n",
    "\n",
    "log.setup()\n",
    "\n",
    "LOGGER = logging.getLogger('Model.Testing')\n",
    "LOGGER.info(\"logger ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20794400-8eed-4153-814a-bd03ae63ed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from datetime import datetime\n",
    "import os\n",
    "from typing import Literal\n",
    "from pprint import pformat\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from generate_train_test import generate_train_test, load_csv\n",
    "from automl import create_automl_model, error_report\n",
    "from serialize import serialize_model, get_tpot_export_code\n",
    "\n",
    "\n",
    "EVAL_RESULT_COL_ORDER = [\n",
    "    'Sport', 'Service', 'Type', 'Style', 'Target', 'R2',\n",
    "    'RMSE', 'MAE', 'ModelType', 'Date', 'Params'\n",
    "]   \n",
    "\n",
    "def log_eval_results(eval_results: list[dict], name: str,\n",
    "                     csv_folder: str = \"eval_results\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    write all evaluation results to csv file in the temp folder and return the dataframe\n",
    "    also write file(s) that describe the final model(s)\n",
    "    \"\"\"\n",
    "    if len(eval_results) == 0:\n",
    "        LOGGER.warn(\"No evaluation results to save\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(eval_results)[EVAL_RESULT_COL_ORDER] \\\n",
    "        .sort_values(['Sport', 'Service', 'Type', 'Style', 'Target', 'ModelType'])\n",
    "    if not os.path.isdir(csv_folder):\n",
    "        os.mkdir(csv_folder)\n",
    "    results_filepath = os.path.join(csv_folder, name + \".csv\")\n",
    "    df.to_csv(results_filepath, index=False)\n",
    "\n",
    "    LOGGER.info(f\"Evaluation results written to '{results_filepath}\")\n",
    "    return df\n",
    "\n",
    "def finalize_error_results(\n",
    "    results: dict, \n",
    "    shared_results_dict, \n",
    "    target, \n",
    "    automl_params, \n",
    "    model_cols,\n",
    "    X_train, y_train,\n",
    "    error_results_folder: str = None\n",
    ") -> dict:\n",
    "    \"\"\" \n",
    "    update and return results to include model information for a single model evaluation \n",
    "\n",
    "    results: dict of results from a single model evaluation from create_automl_model()\n",
    "    \"\"\"\n",
    "    finalized_results = results.copy()\n",
    "    finalized_results['target'] = target\n",
    "    finalized_results['Params'] = automl_params.copy()\n",
    "    if model_cols:\n",
    "        finalized_results['Params']['model_cols'] = model_cols\n",
    "    finalized_results.update(shared_results_dict)\n",
    "\n",
    "    if error_results_folder:\n",
    "        serialize_model(\n",
    "            results['model'], finalized_results['ModelType'], \n",
    "            X_train, y_train, \n",
    "            finalized_results['ModelType'] + \"-\" + target, \n",
    "            model_folder=error_results_folder,\n",
    "            model_desc_folder=error_results_folder,\n",
    "        )   \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaf623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "models that can be evaluated :\n",
    "'all-top' - All available input data predicting the top score\n",
    "'all-lws' - All available input data predicting last winning score\n",
    "'bps-top' - best possible score predicting the top score\n",
    "'bps-lws' - best possible score predicting the last winning score\n",
    "'pca-top' - PCA transformed input data predicting the top score\n",
    "'pca-lws' - PCA transformed input data predicting last winning score\n",
    "\"\"\"\n",
    "_VALID_MODELS = Literal[\n",
    "    'all-top', 'all-lws',\n",
    "    'bps-top', 'bps-lws',\n",
    "    'pca-top', 'pca-lws',\n",
    "]\n",
    "\n",
    "\n",
    "def evaluate_models(\n",
    "    sport, service, style, contest_type, framework, automl_params, pbar,\n",
    "    pca_components=5, data_folder=\"data\",\n",
    "    models_to_test: set[_VALID_MODELS] | None = None\n",
    ") -> tuple[dict, list]:\n",
    "    \"\"\"\n",
    "    models_to_test - set/list of the models to test. if None then all models tested. possible models are\n",
    "    returns tuple of (models, evaluation results)\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    eval_results = []\n",
    "    shared_results_dict = {\n",
    "        'Sport': sport,\n",
    "        'Service': service,\n",
    "        'Style': style.name,\n",
    "        'Type': contest_type.NAME,\n",
    "        'ModelType': framework,\n",
    "        'Date': datetime.now().strftime(\"%Y%m%d\"),\n",
    "    }\n",
    "    df = load_csv(sport, service, style, contest_type, data_folder=data_folder)\n",
    "\n",
    "    model_col_options: list[None | set[str]] = []\n",
    "    if models_to_test is None:\n",
    "        final_models_to_test = {'all-top', 'all-lws',\n",
    "                                'bps-top', 'bps-lws', \n",
    "                                'pca-top', 'pca-lws'}\n",
    "    else:\n",
    "        final_models_to_test = models_to_test\n",
    "\n",
    "    if len(final_models_to_test & {'all-top', 'all-lws', 'pca-top', 'pca-lws'}) > 0:\n",
    "        model_col_options.append(None)\n",
    "    if len(final_models_to_test & {'bps-top', 'bps-lws'}) > 0:\n",
    "        model_col_options.append({'best-possible-score'})\n",
    "\n",
    "    # iterate over the model feature cols\n",
    "    for model_cols in model_col_options:\n",
    "        model_data = generate_train_test(\n",
    "            df,\n",
    "            model_cols=model_cols,\n",
    "            random_state=automl_params['random_state'],\n",
    "        )\n",
    "\n",
    "        if model_data is None or len(model_data[0]) < 5:\n",
    "            LOGGER.error(\n",
    "                \"Not enough training data available! Only found %i cases. Skipping all training\",\n",
    "                len(model_data[0]) if model_data else 0\n",
    "            )\n",
    "            return None, None\n",
    "\n",
    "        (X_train, X_test, y_top_train, y_top_test,\n",
    "         y_last_win_train, y_last_win_test) = model_data\n",
    "\n",
    "        model_ys = [\n",
    "            ('top-score', y_top_train, y_top_test),\n",
    "            ('last-win-score', y_last_win_train, y_last_win_test),\n",
    "        ]\n",
    "\n",
    "        # models for top and last winning score\n",
    "        for target, y_train, y_test in model_ys:\n",
    "            LOGGER.info(\"training model=%s cols=%s\", target, model_cols)\n",
    "            model_desc = f\"{sport}-{service}-{style.name}-{contest_type.NAME}-{target}-{framework}:{model_cols or 'all'}\"\n",
    "            pbar.set_postfix_str(model_desc)\n",
    "\n",
    "            cam_result = create_automl_model(\n",
    "                target,\n",
    "                framework=framework,\n",
    "                X_train=X_train,\n",
    "                y_train=y_train,\n",
    "                X_test=X_test,\n",
    "                y_test=y_test,\n",
    "                model_desc=model_desc,\n",
    "                **automl_params\n",
    "            )\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "            results = finalize_error_results(\n",
    "                cam_result, shared_results_dict, target,\n",
    "                automl_params, model_cols, framework,\n",
    "                X_train, y_train,\n",
    "            )\n",
    "            log_eval_results([results], model_desc)\n",
    "            eval_results.append(results)\n",
    "            models[model_desc] = cam_result['model']\n",
    "\n",
    "        if len({'pca-top', 'pca-lws'} & final_models_to_test) == 0:\n",
    "            continue\n",
    "\n",
    "        # pca models only when using multiple data columns and requested\n",
    "        if model_cols is not None and len(model_cols) == 1:\n",
    "            if model_cols != {'best-possible-score'}:\n",
    "                LOGGER.debug(\n",
    "                    f\"Skipping pca models due to lack of data columns. {model_cols=}\"\n",
    "                )\n",
    "            continue\n",
    "\n",
    "        for target, y_train, y_test in model_ys:\n",
    "            pca_target = target + '-pca'\n",
    "            LOGGER.info(\"training model=%s cols=%s\",\n",
    "                        pca_target, model_cols)\n",
    "            model_desc = f\"{sport}-{service}-{style.name}-{contest_type.NAME}-{pca_target}-{framework}:{model_cols or 'all'}\"\n",
    "            pbar.set_postfix_str(model_desc)\n",
    "            cam_result = create_automl_model(\n",
    "                pca_target,\n",
    "                pca_components=pca_components,\n",
    "                framework=framework,\n",
    "                X_train=X_train, y_train=y_train,\n",
    "                X_test=X_test, y_test=y_test,\n",
    "                model_desc=model_desc,\n",
    "                **automl_params\n",
    "            )\n",
    "            pbar.update()\n",
    "\n",
    "            results = finalize_error_results(\n",
    "                cam_result, shared_results_dict, target,\n",
    "                automl_params, model_cols, framework,\n",
    "                X_train, y_train,\n",
    "            )\n",
    "            results['Params']['n_components'] = pca_components\n",
    "            results['ModelType'] += '-pca'\n",
    "            log_eval_results([results], model_desc)\n",
    "            eval_results.append(results)\n",
    "            models[model_desc] = cam_result['model']\n",
    "\n",
    "    return models, eval_results\n",
    "\n",
    "\n",
    "def multi_run(\n",
    "    model_params: dict, styles, sports, services, contest_types,\n",
    "    models_to_test: set[_VALID_MODELS] | None = None\n",
    "):\n",
    "    LOGGER.info(\"starting multirun\")\n",
    "    models = {}\n",
    "    eval_results = []\n",
    "    progress_total = len(sports) * len(styles) * len(services) * len(contest_types) * len(model_params) * \\\n",
    "        (len(models_to_test) if models_to_test else 6)\n",
    "    pbar = tqdm(total=progress_total, desc=\"Modeling\")\n",
    "    try:\n",
    "        for (\n",
    "            sport, service, style, contest_type, (framework, params)\n",
    "        ) in product(\n",
    "            sports, services, styles, contest_types, model_params.items()\n",
    "        ):\n",
    "            try:\n",
    "                (new_models, new_eval_results) = evaluate_models(\n",
    "                    sport, service, style, contest_type, framework, params, pbar,\n",
    "                    models_to_test=models_to_test)\n",
    "                if new_models is None:\n",
    "                    LOGGER.warning(\"No models generated for %s-%s-%s-%s\",\n",
    "                                   sport, service, style.name, contest_type.NAME)\n",
    "                else:\n",
    "                    models.update(new_models)\n",
    "                    eval_results += new_eval_results\n",
    "            except FileNotFoundError as ex:\n",
    "                LOGGER.error(\n",
    "                    \"Data file required for modeling not found\", exc_info=ex\n",
    "                )\n",
    "    except (Exception, KeyboardInterrupt) as ex:\n",
    "        LOGGER.error(\"Unhandled exception! \", exc_info=ex)\n",
    "        return models, eval_results, ex\n",
    "    LOGGER.info(\"finished multirun\")\n",
    "    return models, eval_results, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3a954a-f170-46a3-88de-3741d31a721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from fantasy_py import ContestStyle\n",
    "from fantasy_py.lineup.strategy import GeneralPrizePool, FiftyFifty\n",
    "\n",
    "SPORTS = ['nhl', 'nfl', 'mlb', 'nba', 'lol']\n",
    "STYLES = [ContestStyle.CLASSIC, ContestStyle.SHOWDOWN]\n",
    "RANDOM_SEED = 0\n",
    "SERVICES = ['fanduel', 'draftkings', 'yahoo']\n",
    "CONTEST_TYPES = [FiftyFifty, GeneralPrizePool]\n",
    "MODELS_TO_TEST = {\n",
    "    'all-top', 'all-lws'\n",
    "}\n",
    "\n",
    "AUTOML_PARAMS = {\n",
    "    'skautoml': {\n",
    "        'per_run_time_limit': 120,\n",
    "        'max_train_time': 900,\n",
    "        'n_jobs': 4,\n",
    "        'random_state': RANDOM_SEED,\n",
    "    },\n",
    "    'tpot': {\n",
    "        'population_size': 100,\n",
    "        'n_jobs': 4,\n",
    "        'verbosity': 2,\n",
    "        'max_train_time': 1200,\n",
    "        'generations': 100,\n",
    "        'early_stop': 15,\n",
    "        'template': 'Selector-Transformer-Regressor',\n",
    "        'random_state': RANDOM_SEED,\n",
    "    }\n",
    "}\n",
    "\n",
    "(models, eval_results, unhandled_exception) = multi_run(\n",
    "    AUTOML_PARAMS, STYLES, SPORTS, SERVICES, CONTEST_TYPES, models_to_test=MODELS_TO_TEST\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93df427",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_col_order = [\n",
    "    'Sport', 'Service', 'Type', 'Style', 'Target', 'R2',\n",
    "    'RMSE', 'MAE', 'ModelType', 'Date', 'Params'\n",
    "]\n",
    "if len(eval_results):\n",
    "    eval_results_df = pd.DataFrame(eval_results)[eval_result_col_order] \\\n",
    "        .sort_values(['Sport', 'Service', 'Type', 'Style', 'Target', 'ModelType'])\n",
    "\n",
    "    eval_results_df = log_eval_results(\n",
    "        eval_results,\n",
    "        \"all_eval_results\"\n",
    "    )\n",
    "\n",
    "    if unhandled_exception:\n",
    "        import traceback\n",
    "        print(traceback.format_exc(limit=None, chain=True))\n",
    "\n",
    "    with pd.option_context(\n",
    "        'display.max_rows', 1000,\n",
    "        'display.max_columns', 100,\n",
    "        'display.max_colwidth', None\n",
    "    ):\n",
    "        display(eval_results_df)\n",
    "\n",
    "    print(eval_results_df.to_csv(index=False, sep=\"\\t\"))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "006d5deb8e6cdcd4312641bdf15f3bc20f0769a7305d81173599a7b40f33b4a2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "006d5deb8e6cdcd4312641bdf15f3bc20f0769a7305d81173599a7b40f33b4a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
