{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "585d8aeb-9ef2-471c-a683-fb65c12007d9",
   "metadata": {},
   "source": [
    "# Model Testing for Daily Fantasy Scores\n",
    "Predict for the minimum and maximum winning scores for a slate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354471f7-c002-40cd-b71f-f8fec169819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "FORMAT = '%(asctime)-15s :: %(message)s'\n",
    "logging.basicConfig(format=FORMAT)\n",
    "LOGGER = logging.getLogger('dfscore')\n",
    "LOGGER.setLevel(logging.INFO)\n",
    "LOGGER.info(\"logger ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20794400-8eed-4153-814a-bd03ae63ed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from generate_train_test import generate_train_test, load_csv\n",
    "from automl import create_automl_model, error_report\n",
    "\n",
    "\n",
    "EVAL_RESULT_COL_ORDER = [\n",
    "    'Sport', 'Service', 'Type', 'Style', 'y', 'R2',\n",
    "    'RMSE', 'MAE', 'ModelType', 'Date', 'Params'\n",
    "]\n",
    "\n",
    "def eval_results_to_csv(eval_results: list[dict], model_name: str, \n",
    "                        csv_folder: str = \"eval_results\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    write all evaluation results to csv file in the temp folder and return the dataframe\n",
    "    \"\"\"\n",
    "    if len(eval_results) == 0:\n",
    "        LOGGER.warn(\"No evaluation results to save\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(eval_results)[EVAL_RESULT_COL_ORDER] \\\n",
    "        .sort_values(['Sport', 'Service', 'Type', 'Style', 'y', 'ModelType'])\n",
    "    if not os.path.isdir(csv_folder):\n",
    "        os.mkdir(csv_folder)\n",
    "    results_filepath = os.path.join(csv_folder, model_name + \".csv\")\n",
    "    df.to_csv(results_filepath, index=False)\n",
    "    LOGGER.info(f\"Evaluation results written to '{results_filepath}'\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def finalize_error_results(results, shared_results_dict, model_name, automl_params, model_cols) -> dict:\n",
    "    \"\"\" update and return results to include model information \"\"\"\n",
    "    results['y'] = model_name.split('-')[0]\n",
    "    results['Params'] = dict(automl_params)\n",
    "    if model_cols:\n",
    "        results['Params']['model_cols'] = model_cols\n",
    "    results.update(shared_results_dict)\n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_models(\n",
    "    sport, service, style, contest_type, framework, automl_params,\n",
    "    pca_components=5, seed=0, data_folder=\"data\"\n",
    ") -> tuple[dict, list]:\n",
    "    \"\"\"\n",
    "    evaluate and return 6 models\n",
    "    for the top score and last winning score evaluate \n",
    "    1) a standard model using all features, \n",
    "    2) a pca model that reduces the features to pca_components components\n",
    "    3) a model based on best score\n",
    "\n",
    "    returns tuple of (models, evaluation results)\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    eval_results = []\n",
    "    shared_results_dict = {\n",
    "        'Sport': sport,\n",
    "        'Service': service,\n",
    "        'Style': style.name,\n",
    "        'Type': contest_type.NAME,\n",
    "        'ModelType': framework,\n",
    "        'Date': datetime.now().strftime(\"%Y%m%d\"),\n",
    "    }\n",
    "    df = load_csv(sport, service, style, contest_type, data_folder=data_folder)\n",
    "    # with pd.option_context('max_rows', 1000, 'max_columns', 100):\n",
    "    #     LOGGER.info(f\"{len(df)} rows\")\n",
    "    # display(df)\n",
    "\n",
    "    # generate 6 models, top and last winning score models\n",
    "    # using 1) all data columns, 2) pca reduction of all data and 3) just the best possible score\n",
    "    for model_cols in [None, {'best-possible-score'}]:\n",
    "        model_data = generate_train_test(\n",
    "            df,\n",
    "            model_cols=model_cols,\n",
    "            random_state=seed,\n",
    "        )\n",
    "\n",
    "        if model_data is None or len(model_data[0]) < 5:\n",
    "            LOGGER.error(\"Not enough training data available!\")\n",
    "            return None, None\n",
    "\n",
    "        (X_train, X_test, y_top_train, y_top_test,\n",
    "         y_last_win_train, y_last_win_test) = model_data\n",
    "\n",
    "        model_ys = [\n",
    "            ('top-score', y_top_train, y_top_test),\n",
    "            ('last-win-score', y_last_win_train, y_last_win_test),\n",
    "        ]\n",
    "        # models for top and last winning score\n",
    "        for model_name, y_train, y_test in model_ys:\n",
    "            LOGGER.info(\"training model=%s cols=%s\", model_name, model_cols)\n",
    "            model, fit_params = create_automl_model(\n",
    "                model_name,\n",
    "                seed=seed,\n",
    "                framework=framework,\n",
    "                **automl_params\n",
    "            )\n",
    "            model.fit(X_train, y_train, **fit_params)\n",
    "            model_desc = f\"{sport}-{service}-{style.name}-{contest_type.NAME}-{model_name}-{framework}-ftrs:{model_cols}\"\n",
    "\n",
    "            results = error_report(model, X_test, y_test, model_desc)\n",
    "            results = finalize_error_results(\n",
    "                results, shared_results_dict, model_name,\n",
    "                automl_params, model_cols\n",
    "            )\n",
    "            eval_results_to_csv([results], model_desc)\n",
    "            eval_results.append(results)\n",
    "\n",
    "            models[model_desc] = model\n",
    "\n",
    "        # pca models only when using multiple data columns\n",
    "        if model_cols is not None and len(model_cols) == 1:\n",
    "            # return\n",
    "            continue\n",
    "\n",
    "        for model_name, y_train, y_test in model_ys:\n",
    "            pca_model_name = model_name + '-pca'\n",
    "            LOGGER.info(\"training model=%s cols=%s\",\n",
    "                        pca_model_name, model_cols)\n",
    "            model, fit_params = create_automl_model(\n",
    "                pca_model_name,\n",
    "                pca_components=pca_components,\n",
    "                seed=seed,\n",
    "                framework=framework,\n",
    "                **automl_params\n",
    "            )\n",
    "            model.fit(X_train, y_train, **fit_params)\n",
    "            model_desc = f\"{sport}-{service}-{style.name}-{contest_type.NAME}-{pca_model_name}-{framework}: {model_cols=}\"\n",
    "\n",
    "            results = error_report(model, X_test, y_test, model_desc)\n",
    "            results = finalize_error_results(\n",
    "                results, shared_results_dict, model_name,\n",
    "                automl_params, model_cols\n",
    "            )\n",
    "            results['Params']['n_components'] = pca_components\n",
    "            results['ModelType'] += '-pca'\n",
    "            eval_results_to_csv([results], model_desc)\n",
    "            eval_results.append(results)\n",
    "\n",
    "            models[model_desc] = model\n",
    "\n",
    "    return models, eval_results\n",
    "\n",
    "\n",
    "def multi_run(model_params: dict, styles, sports, services, contest_types, seed):\n",
    "    LOGGER.info(\"starting multirun\")\n",
    "    models = {}\n",
    "    eval_results = []\n",
    "    try:\n",
    "        for framework, params in model_params.items():\n",
    "            for (sport, service, style, contest_type) in product(sports, services, styles, contest_types):\n",
    "                try:\n",
    "                    (new_models, new_eval_results) = evaluate_models(\n",
    "                        sport, service, style, contest_type, framework, params, seed=seed)\n",
    "                    if new_models is None:\n",
    "                        LOGGER.warning(\"No models generated for %s-%s-%s-%s\",\n",
    "                                       sport, service, style.name, contest_type.NAME)\n",
    "                    else:\n",
    "                        models.update(new_models)\n",
    "                        eval_results += new_eval_results\n",
    "                except FileNotFoundError as ex:\n",
    "                    LOGGER.error(\n",
    "                        \"Data file required for modeling not found\", exc_info=ex)\n",
    "    except (Exception, KeyboardInterrupt) as ex:\n",
    "        LOGGER.error(\"Unhandled exception! \", exc_info=ex)\n",
    "        return models, eval_results, ex\n",
    "    LOGGER.info(\"finished multirun\")\n",
    "    return models, eval_results, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3a954a-f170-46a3-88de-3741d31a721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from fantasy_py import ContestStyle\n",
    "from fantasy_py.lineup.strategy import GeneralPrizePool, FiftyFifty\n",
    "\n",
    "# normal test run\n",
    "STYLES = [ContestStyle.CLASSIC, ContestStyle.SHOWDOWN]\n",
    "SPORTS = ['nhl', 'nfl', 'mlb', 'nba', 'lol']\n",
    "RANDOM_SEED = 0\n",
    "SERVICES = ['fanduel', 'draftkings', 'yahoo']\n",
    "CONTEST_TYPES = [FiftyFifty, GeneralPrizePool]\n",
    "\n",
    "AUTOML_PARAMS = {\n",
    "    # 'skautoml': {\n",
    "    #     'per_run_time_limit': 120,\n",
    "    #     'max_train_time': 600,\n",
    "    #     'n_jobs': 4,\n",
    "    # },\n",
    "    'tpot': {\n",
    "        'population_size': 100, \n",
    "        'n_jobs': 4,\n",
    "        'verbosity': 2,\n",
    "        'max_train_time': 1200,\n",
    "        'generations': 100,\n",
    "        'early_stop': 10,\n",
    "        'template': 'Selector-Transformer-Regressor',\n",
    "    }\n",
    "}\n",
    "\n",
    "(models, eval_results, unhandled_exception) = multi_run(AUTOML_PARAMS, STYLES, SPORTS, SERVICES, CONTEST_TYPES, RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93df427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(models)\n",
    "eval_result_col_order = [\n",
    "    'Sport', 'Service', 'Type', 'Style', 'y', 'R2',\n",
    "    'RMSE', 'MAE', 'ModelType', 'Date', 'Params'\n",
    "]\n",
    "eval_results_df = pd.DataFrame(eval_results)[eval_result_col_order] \\\n",
    "    .sort_values(['Sport', 'Service', 'Type', 'Style', 'y', 'ModelType'])\n",
    "\n",
    "eval_results_df = eval_results_to_csv(\n",
    "    eval_results,\n",
    "    \"all_eval_results.csv\")\n",
    "\n",
    "if unhandled_exception:\n",
    "    import traceback\n",
    "    print(traceback.format_exc(limit=None, chain=True))\n",
    "\n",
    "with pd.option_context(\n",
    "    'display.max_rows', 1000,\n",
    "    'display.max_columns', 100,\n",
    "    'display.max_colwidth', None\n",
    "):\n",
    "    display(eval_results_df)\n",
    "\n",
    "print(eval_results_df.to_csv(index=False, sep=\"\\t\"))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "006d5deb8e6cdcd4312641bdf15f3bc20f0769a7305d81173599a7b40f33b4a2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "006d5deb8e6cdcd4312641bdf15f3bc20f0769a7305d81173599a7b40f33b4a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
