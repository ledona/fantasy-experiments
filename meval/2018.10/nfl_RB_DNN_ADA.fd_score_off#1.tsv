{"calc_name": "keras", "calc_params": {"calc_stats": {"cur_opp_team_stats": ["def_block_fg", "def_block_punt", "def_block_xp", "def_fumble_recov", "def_fumble_recov_tds", "def_int", "def_int_tds", "def_sacks", "def_safety", "def_tds", "op_passing_yds", "op_pts", "op_rushing_yds", "op_turnovers", "op_yds", "pen_yds", "pens"], "extra_stats": ["home_C", "player_home_H", "team_home_H", "team_win"], "model_player_stat": "fd_score_off#1", "model_team_stat": null, "player_stats": ["fumbles_lost", "receiving_rec", "receiving_targets", "receiving_tds", "receiving_twoptm", "receiving_yds", "rushing_att", "rushing_tds", "rushing_twoptm", "rushing_yds", "tds"], "prev_opp_team_stats": [], "team_stats": ["passing_yds", "pts", "rushing_yds", "turnovers"]}, "impute": true, "models_path": "./MODELS_keras", "normalize": true, "player_pos": ["RB"]}, "datetime_utc": "20181029 020355", "db_id": 1, "db_path": "nfl.db", "fantasy_version": "v0.31.0-1-gb300643", "filename_prefix": "nfl_RB_DNN_ADA", "folds": 2, "hyper_dists": {"activation": {"cls": "HPCategoricalDist", "name": "activation", "values": ["linear", "relu", "tanh", "sigmoid"]}, "dropout": {"cls": "HPNumericDist", "dist_type": "float", "high": 0.7, "low": 0.3, "max_float_percision": 10, "name": "dropout"}, "hist_agg": {"cls": "HPConstantDist", "name": "hist_agg", "value": "none"}, "layers": {"cls": "HPNumericDist", "dist_type": "int", "high": 5, "increment": 1, "low": 1, "name": "layers"}, "learning_method": {"cls": "HPCategoricalDist", "name": "learning_method", "values": ["adagrad", "adadelta", "adam", "adamax", "nadam"]}, "n_cases": {"cls": "HPNumericDist", "dist_type": "int", "high": 3500, "increment": 1, "low": 100, "name": "n_cases"}, "n_features": {"cls": "HPConstantDist", "name": "n_features", "value": null}, "n_games": {"cls": "HPNumericDist", "dist_type": "int", "high": 7, "increment": 1, "low": 1, "name": "n_games"}, "steps": {"cls": "HPNumericDist", "dist_type": "int", "high": 1000, "increment": 100, "low": 100, "name": "steps"}, "units": {"cls": "HPNumericDist", "dist_type": "int", "high": 100, "increment": 1, "low": 20, "name": "units"}}, "random_seed": 791439706, "resume_datetimes": null, "scoring": ["mae", "r2"], "search": {"bayes_init_pts": 7, "bayes_retry_cache": true, "iterations": 70, "method": "bayes", "pretend": false}, "search_bayes_scoring_method": "mae", "season_parts": ["REG"], "seasons": [2017, 2016, 2015, 2014, 2013, 2012, 2011, 2010, 2009]}
score_mae	score_r2	activation	dropout	hist_agg	layers	learning_method	n_cases	n_features	n_games	steps	units
-4.5893171	0.282282	linear	0.5107574153	none	2	adadelta	1987	106	3	200	72
-4.5955343	0.2725635	linear	0.4393532358	none	1	adagrad	3314	36	1	400	66
-4.6506447	0.2865149	linear	0.4444880692	none	4	adamax	3265	141	4	500	66
-4.6949196	0.1999929	tanh	0.4706555534	none	2	adamax	363	71	2	500	45
-5.5225225	0.1028666	linear	0.3909091361	none	2	adam	410	141	4	900	63
-4.8085717	0.2342582	sigmoid	0.6968034679	none	3	adadelta	3235	176	5	700	97
-4.7345465	0.2492062	sigmoid	0.4368770934	none	5	adagrad	2958	71	2	300	49
-4.824657	0.0597415	linear	0.7	none	1	adagrad	100	36	1	100	100
-5.0539082	-0.0789691	tanh	0.7	none	1	adadelta	100	36	1	100	51
-4.8386583	0.2457795	sigmoid	0.4642126072	none	1	adadelta	3500	176	5	900	100
-4.6509946	0.2570242	linear	0.3	none	3	adagrad	2973	36	1	100	20
-4.6608512	0.2592694	linear	0.5390188772	none	1	adagrad	3500	36	1	600	100
-4.6745908	0.2933083	linear	0.5024077935	none	1	adadelta	2131	106	3	300	84
-4.5769943	0.2599907	linear	0.3	none	5	adagrad	3331	36	1	100	20
-4.9181977	0.2061047	linear	0.7	none	5	adadelta	3500	36	1	100	20
-4.5669801	0.2960772	linear	0.5160074296	none	3	adadelta	1857	106	3	100	62
-4.952034	0.192472	sigmoid	0.4002803763	none	2	adadelta	1849	211	6	600	45
-4.7227993	0.2694207	linear	0.3691521401	none	5	adagrad	3260	106	3	100	20
-4.8231581	0.2529242	relu	0.658323181	none	1	nadam	1821	36	1	800	20
-4.7032757	0.2918968	linear	0.5490381718	none	4	adadelta	1857	106	3	500	78
-5.4939725	0.0910117	linear	0.6555401105	none	5	adadelta	640	106	3	100	20
-4.5893115	0.2671851	linear	0.487604688	none	2	adadelta	2020	106	3	100	67
-4.5995404	0.2799269	linear	0.5672788283	none	4	adamax	2281	106	3	500	54
-4.9766044	0.2080769	tanh	0.4602271439	none	1	adamax	2582	141	4	500	62
-4.6104861	0.2271484	linear	0.3	none	5	adamax	100	36	1	500	20
-4.6978516	0.2474858	linear	0.3886902517	none	4	adamax	1203	36	1	700	88
-5.515222	-0.0614646	linear	0.4101172104	none	1	adamax	262	176	5	100	38
-4.7653891	0.2739898	linear	0.4909547547	none	2	adamax	2041	106	3	100	65
-4.6781684	0.2878472	linear	0.5499214777	none	2	adamax	2603	141	4	1000	74
-4.708964	0.280733	linear	0.6851363682	none	2	adamax	2464	106	3	700	36
-4.5534028	0.3105882	linear	0.5470526721	none	5	adamax	2312	106	3	600	63
-4.8166199	0.0351755	tanh	0.347659625	none	5	adamax	192	36	1	800	47
-4.994607	0.1948227	relu	0.4667759218	none	1	adadelta	1850	106	3	100	73
-4.6174738	0.2622053	linear	0.514986368	none	1	adadelta	1851	106	3	100	58
-5.8548423	-0.0949619	tanh	0.3	none	3	adamax	311	176	5	1000	80
-4.75563	0.2542611	linear	0.3	none	5	adamax	3500	36	1	100	20
-4.6298312	0.3051896	linear	0.4978357964	none	5	adam	2368	106	3	600	96
-4.677845	0.2855151	linear	0.3631625544	none	5	adadelta	1993	106	3	100	30
-4.655715	0.2702226	linear	0.5735328201	none	5	adamax	2436	106	3	800	67
-4.8296875	0.2046328	sigmoid	0.5146792882	none	4	adamax	2169	106	3	900	53
-4.607187	0.3012262	linear	0.6922585637	none	4	adadelta	1997	106	3	100	60
-4.6921745	0.2935518	linear	0.5385407339	none	5	adamax	2207	106	3	400	61
-4.7418376	0.2582823	linear	0.4880065149	none	5	adagrad	3123	36	1	100	55
-4.6472576	0.2806559	linear	0.3	none	5	adadelta	2084	71	2	100	73
-4.6627209	0.2909541	linear	0.4864413274	none	5	adagrad	3500	246	7	100	51
-4.7036761	0.2872313	linear	0.3	none	5	adam	3500	246	7	100	47
-4.6095857	0.2132644	linear	0.6972881416	none	5	adagrad	3500	211	6	100	100
-4.6472446	0.281812	linear	0.5687160639	none	5	adagrad	3500	211	6	200	39
-4.665735	0.1870128	linear	0.7	none	5	adagrad	3126	211	6	100	100
-4.6583149	0.2895387	linear	0.7	none	5	adamax	3500	211	6	100	100
-4.6279146	0.2601054	linear	0.3	none	1	adagrad	3426	36	1	1000	20
-4.79659	0.2617362	linear	0.3	none	4	adamax	1572	36	1	300	52
-4.613323	0.2677669	linear	0.3	none	1	adagrad	3500	36	1	1000	20
-4.613323	0.2677669	linear	0.3	none	1	adagrad	3500	36	1	1000	20
-4.613323	0.2677669	linear	0.3	none	1	adagrad	3500	36	1	1000	20
-4.613323	0.2677669	linear	0.3	none	1	adagrad	3500	36	1	1000	20
-4.613323	0.2677669	linear	0.3	none	1	adagrad	3500	36	1	1000	20
-4.57944	0.2779995	sigmoid	0.7	none	2	adadelta	1934	106	3	100	63
-4.6333055	0.2791479	linear	0.4318048998	none	5	adamax	2329	106	3	600	40
-4.6749472	0.2805503	linear	0.6888384851	none	5	adamax	2311	106	3	600	82
-4.5386411	0.296152	linear	0.7	none	4	adagrad	3500	211	6	500	100
-4.5916533	0.2944531	linear	0.7	none	1	adagrad	3500	211	6	1000	100
-4.5916533	0.2944531	linear	0.7	none	1	adagrad	3500	211	6	1000	100
-4.569834	0.2980985	linear	0.7	none	2	adagrad	3500	211	6	600	100
-4.6456251	0.288155	linear	0.7	none	2	adagrad	3500	176	5	1000	67
-4.5831866	0.3028969	linear	0.7	none	3	adagrad	3500	211	6	1000	100
-4.7057737	0.2861739	linear	0.7	none	4	adagrad	3500	246	7	700	68
-4.6108363	0.2809692	linear	0.7	none	3	adagrad	3500	211	6	800	84
-4.6071442	0.280914	linear	0.7	none	3	adagrad	3500	211	6	800	100
-4.5916533	0.2944531	linear	0.7	none	1	adagrad	3500	211	6	1000	100
