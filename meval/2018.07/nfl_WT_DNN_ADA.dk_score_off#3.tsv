{"calc_name": "keras", "calc_params": {"calc_stats": {"cur_opp_team_stats": ["def_block_fg", "def_block_punt", "def_block_xpt", "def_fumble_recov", "def_fumble_recov_tds", "def_int", "def_int_tds", "def_sacks", "def_safety", "def_tds", "op_passing_yds", "op_pts", "op_rushing_yds", "op_turnovers", "op_yds"], "extra_stats": [], "model_player_stat": "dk_score_off#3", "model_team_stat": null, "player_stats": ["fumbles_lost", "receiving_rec", "receiving_tds", "receiving_twoptm", "receiving_yds", "tds"], "prev_opp_team_stats": [], "team_stats": ["passing_yds", "pts", "rushing_yds", "turnovers"]}, "impute": true, "models_path": "/Users/delano/working/fantasy/MODELS_keras", "normalize": true, "player_pos": ["TE", "WR"]}, "datetime_utc": "20180725 142724", "db_id": 1, "db_path": "nfl.db", "fantasy_version": "v0.28.3-31-g1679bfa5", "filename_prefix": "nfl_WT_DNN_ADA", "folds": 2, "hyper_dists": {"activation": {"cls": "HPCategoricalDist", "name": "activation", "values": ["linear", "relu", "tanh", "sigmoid"]}, "dropout": {"cls": "HPNumericDist", "dist_type": "float", "high": 0.7, "low": 0.3, "max_float_percision": 10, "name": "dropout"}, "hist_agg": {"cls": "HPConstantDist", "name": "hist_agg", "value": "none"}, "layers": {"cls": "HPNumericDist", "dist_type": "int", "high": 5, "increment": 1, "low": 1, "name": "layers"}, "learning_method": {"cls": "HPCategoricalDist", "name": "learning_method", "values": ["adagrad", "adadelta", "adam", "adamax", "nadam"]}, "n_cases": {"cls": "HPNumericDist", "dist_type": "int", "high": 9000, "increment": 1, "low": 100, "name": "n_cases"}, "n_features": {"cls": "HPConstantDist", "name": "n_features", "value": null}, "n_games": {"cls": "HPNumericDist", "dist_type": "int", "high": 7, "increment": 1, "low": 1, "name": "n_games"}, "steps": {"cls": "HPNumericDist", "dist_type": "int", "high": 1000, "increment": 100, "low": 100, "name": "steps"}, "units": {"cls": "HPNumericDist", "dist_type": "int", "high": 100, "increment": 1, "low": 20, "name": "units"}}, "random_seed": 1757813083, "resume_datetimes": null, "scoring": ["mae", "r2"], "search": {"bayes_init_pts": 7, "bayes_retry_cache": true, "iterations": 70, "method": "bayes", "pretend": false}, "search_bayes_scoring_method": "mae", "season_parts": ["REG"], "seasons": [2017, 2016, 2015, 2014, 2013, 2012, 2011, 2010]}
score_mae	score_r2	activation	dropout	hist_agg	layers	learning_method	n_cases	n_features	n_games	steps	units
-5.203877	0.1572752	linear	0.6090875645	none	1	adamax	1146	75	3	200	90
-5.1581265	0.1603708	sigmoid	0.4292193164	none	5	adadelta	6186	125	5	800	30
-5.2100971	0.1746686	relu	0.3367496345	none	2	adagrad	6481	75	3	700	65
-5.0318101	0.2191541	linear	0.3554692075	none	1	adamax	8327	75	3	700	97
-5.2982303	0.0922924	tanh	0.3419905212	none	5	nadam	5023	25	1	700	51
-5.23692	0.158098	sigmoid	0.6674096573	none	3	nadam	4663	150	6	700	55
-5.0790853	0.2072243	tanh	0.6178830075	none	3	adamax	7288	100	4	300	28
-5.3330457	0.1667266	sigmoid	0.7	none	5	adamax	9000	175	7	1000	20
-5.1135744	0.1892293	linear	0.3	none	1	adamax	9000	50	2	1000	100
-5.372783	0.0196057	relu	0.6012961976	none	3	adamax	9000	100	4	300	87
-5.0544301	0.1952449	linear	0.3425183747	none	2	adamax	5593	50	2	900	95
-5.0188576	0.2354153	linear	0.3773608408	none	1	adamax	9000	125	5	400	95
-5.2982641	0.1369292	linear	0.3967116554	none	1	adam	7960	25	1	400	63
-5.0496426	0.2117972	linear	0.3938187231	none	1	nadam	9000	75	3	400	100
-4.9654127	0.2360697	linear	0.3960625052	none	1	adagrad	9000	125	5	100	77
-4.987231	0.242269	linear	0.3862024083	none	1	adagrad	9000	175	7	100	68
-5.2540682	0.1386393	linear	0.4529616736	none	2	adagrad	7288	25	1	1000	22
-5.274751	0.1504136	sigmoid	0.389473558	none	1	adadelta	7592	25	1	100	77
-5.2653297	0.144151	linear	0.3725205626	none	1	adadelta	8308	25	1	500	78
-5.1102146	0.1648097	tanh	0.7	none	1	adamax	2235	100	4	200	70
-5.880337	-0.2168288	linear	0.3724252106	none	1	nadam	100	25	1	300	78
-4.9751033	0.2405797	linear	0.3	none	1	adamax	6358	125	5	100	100
-4.9933575	0.2309442	sigmoid	0.4745230855	none	2	adagrad	9000	125	5	400	85
-5.1035936	0.2071038	tanh	0.7	none	1	adamax	6148	100	4	1000	20
-6.0613235	-0.2373445	relu	0.7	none	5	nadam	100	175	7	300	27
-5.0833575	0.1971452	linear	0.449956765	none	1	adagrad	9000	50	2	100	100
-5.0071	0.2338372	linear	0.3	none	1	adagrad	7669	125	5	100	100
-4.9659698	0.2396838	linear	0.3954796548	none	1	adagrad	9000	125	5	100	100
-5.0636663	0.2195046	linear	0.3	none	1	adagrad	7641	100	4	100	100
-4.994821	0.2364164	linear	0.3	none	1	adagrad	8957	150	6	100	100
-4.9563881	0.236803	linear	0.5306317723	none	1	adagrad	9000	125	5	100	69
-5.2593815	0.1630802	tanh	0.3	none	5	adagrad	5953	75	3	100	100
-4.9230629	0.2289226	linear	0.7	none	1	adagrad	7963	175	7	100	20
-4.9351794	0.2344282	linear	0.7	none	1	adagrad	7900	175	7	400	31
-4.9473558	0.2318576	linear	0.5878995922	none	1	adagrad	7716	175	7	100	20
-5.048013	0.1932747	linear	0.7	none	3	adagrad	8401	150	6	100	20
-5.0259092	0.2312378	sigmoid	0.7	none	1	adagrad	7873	175	7	200	98
-4.9321514	0.231796	linear	0.7	none	1	adagrad	7940	175	7	200	21
-4.8993705	0.2235227	linear	0.7	none	1	adagrad	7995	150	6	100	20
-5.020151	0.2262335	sigmoid	0.7	none	1	adagrad	8836	150	6	800	20
-4.925957	0.2300798	linear	0.7	none	2	adagrad	7923	125	5	100	54
-4.9829354	0.2142993	linear	0.7	none	2	adagrad	9000	75	3	100	80
-4.9225406	0.237744	linear	0.7	none	1	adagrad	7882	150	6	100	79
-4.9800072	0.2060723	linear	0.7	none	2	adagrad	7964	125	5	100	20
-4.9464438	0.235504	linear	0.7	none	1	adagrad	9000	125	5	100	100
-5.0300777	0.2234996	linear	0.7	none	4	adamax	7016	100	4	100	100
-4.9235868	0.2322084	linear	0.7	none	1	adagrad	7819	150	6	100	38
-5.2675118	0.1221741	linear	0.7	none	5	adagrad	6563	25	1	100	100
-4.9788438	0.2336872	linear	0.7	none	2	adagrad	7967	150	6	900	24
-5.3407047	0.0962048	sigmoid	0.7	none	5	adagrad	4524	25	1	100	59
-4.9930666	0.2327603	linear	0.7	none	1	nadam	7835	150	6	100	38
-4.938282	0.2274028	linear	0.7	none	1	adagrad	5784	150	6	100	54
-5.1052597	0.1831114	sigmoid	0.3	none	5	adamax	9000	50	2	100	100
-5.058168	0.2131725	linear	0.7	none	1	adagrad	4232	175	7	100	100
-4.9874686	0.2229337	linear	0.6484992293	none	1	adagrad	5347	125	5	100	100
-5.0352017	0.2154039	linear	0.3	none	4	adagrad	6921	100	4	100	98
-5.018703	0.2309691	linear	0.7	none	1	adagrad	6648	150	6	100	100
-4.9333218	0.2267439	linear	0.7	none	2	adagrad	7474	125	5	100	62
-4.960953	0.2309135	linear	0.7	none	2	adagrad	7495	125	5	100	62
-4.9271396	0.2278409	linear	0.7	none	1	adagrad	7477	125	5	100	50
-4.9566194	0.2339639	linear	0.7	none	1	adagrad	7426	125	5	100	49
-4.9666121	0.2228246	linear	0.7	none	1	adagrad	7538	125	5	100	53
-4.9760611	0.224494	linear	0.7	none	2	adagrad	9000	100	4	100	100
-4.9421942	0.2299503	linear	0.7	none	1	adagrad	7330	150	6	100	47
-4.973327	0.2253148	linear	0.7	none	1	adagrad	7539	125	5	100	51
-4.953013	0.2306112	linear	0.7	none	1	adagrad	7414	125	5	100	50
-4.9121665	0.237482	linear	0.7	none	1	adagrad	7467	125	5	100	51
-4.9760611	0.224494	linear	0.7	none	2	adagrad	9000	100	4	100	100
-4.9787529	0.2282195	linear	0.7	none	2	adagrad	9000	100	4	100	87
-4.976829	0.2275519	linear	0.7	none	2	adagrad	9000	100	4	100	86
